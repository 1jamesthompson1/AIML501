@inproceedings{abbeelApprenticeshipLearningInverse2004,
  title = {Apprenticeship Learning via Inverse Reinforcement Learning},
  booktitle = {Proceedings of the Twenty-First International Conference on {{Machine}} Learning},
  author = {Abbeel, Pieter and Ng, Andrew Y.},
  year = 2004,
  month = jul,
  series = {{{ICML}} '04},
  pages = {1},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1015330.1015430},
  urldate = {2025-12-13},
  abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
  isbn = {978-1-58113-838-2}
}

@article{adamowiczCombiningRevealedStated1994,
  title = {Combining {{Revealed}} and {{Stated Preference Methods}} for {{Valuing Environmental Amenities}}},
  author = {Adamowicz, W. and Louviere, J. and Williams, M.},
  year = 1994,
  month = may,
  journal = {Journal of Environmental Economics and Management},
  volume = {26},
  number = {3},
  pages = {271--292},
  issn = {0095-0696},
  doi = {10.1006/jeem.1994.1017},
  urldate = {2025-12-14},
  abstract = {A stated preference model and a revealed preference model for recreational site choice are examined and compared. Both models are based on random utility theory and the data are obtained from the same individuals. The stated preference model is based on the respondent{$\prime$}s choice from hypothetical choice sets. Attributes in the stated preference model are based on the ranges of the actual levels of attributes in the revealed preference choice set and are presented to respondents using a fractional factorial statistical design. The results show that while independently estimated models appear to reflect different underlying preferences, joint estimation of the model parameters, including estimation of the relative scale parameter, provides evidence that the underlying preferences are in fact similar. Furthermore, combining the revealed and stated preference information yields other benefits in estimation.},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/PKVCE6YW/Adamowicz et al. - 1994 - Combining Revealed and Stated Preference Methods for Valuing Environmental Amenities.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/Y7S9438X/S0095069684710175.html}
}

@misc{AISafetyParadox,
  title = {The {{AI Safety Paradox}}: {{When}} '{{Safe}}' {{AI Makes Systems More Dangerous}}},
  shorttitle = {The {{AI Safety Paradox}}},
  journal = {The Collective Intelligence Project},
  urldate = {2025-11-18},
  abstract = {AI safety is becoming an established field and science. Yet by focusing only on the safety of individual models, AI labs may actually be making the conditions in which they're deployed less safe.},
  howpublished = {https://www.cip.org/blog/safetyparadox},
  langid = {canadian},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/JTCM6AH2/safetyparadox.html}
}

@misc{AIsLeversAre,
  title = {{{AIs}} without Levers Are Inert - by {{James Padolsey}}},
  urldate = {2025-11-21},
  howpublished = {https://blog.j11y.io/2024-07-11\_AIs\_inert/},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/LTBNYY2E/2024-07-11_AIs_inert.html}
}

@misc{alayracFlamingoVisualLanguage2022,
  title = {Flamingo: A {{Visual Language Model}} for {{Few-Shot Learning}}},
  shorttitle = {Flamingo},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  year = 2022,
  month = nov,
  number = {arXiv:2204.14198},
  eprint = {2204.14198},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.14198},
  urldate = {2025-12-07},
  abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/CU9J3PZN/Alayrac et al. - 2022 - Flamingo a Visual Language Model for Few-Shot Learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/S56GCF98/2204.html}
}

@book{amesAnalectsConfuciusPhilosophical1998,
  title = {The {{Analects}} of {{Confucius}}: {{A Philosophical Translation}}},
  shorttitle = {The {{Analects}} of {{Confucius}}},
  author = {Ames, Roger T. and Jr, Henry Rosemont},
  year = 1998,
  publisher = {Random House Publishing Group},
  abstract = {"To quietly persevere in storing up what is learned, to continue studying without respite, to instruct others without growing weary--is this not me?"--ConfuciusConfucius is recognized as China's first and greatest teacher, and his ideas have been the fertile soil in which the Chinese cultural tradition has flourished. Now, here is a translation of the recorded thoughts and deeds that best remember Confucius--informed for the first time by the manuscript version found at Dingzhou in 1973, a partial text dating to 55 BCE and only made available to the scholarly world in 1997. The earliest Analects yet discovered, this work provides us with a new perspective on the central canonical text that has defined Chinese culture--and clearly illuminates the spirit and values of Confucius.Confucius (551-479 BCE) was born in the ancient state of Lu into an era of unrelenting, escalating violence as seven of the strongest states in the proto-Chinese world warred for supremacy. The landscape was not only fierce politically but also intellectually. Although Confucius enjoyed great popularity as a teacher, and many of his students found their way into political office, he personally had little influence in Lu. And so he began to travel from state to state as an itinerant philosopher to persuade political leaders that his teachings were a formula for social and political success. Eventually, his philosophies came to dictate the standard of behavior for all of society--including the emperor himself.Based on the latest research and complete with both Chinese and English texts, this revealing translation serves both as an excellent introduction to Confucian thought and as an authoritative addition to sophisticated debate.},
  googlebooks = {ulEnpjoqwTwC},
  isbn = {978-0-307-77571-9},
  langid = {english},
  keywords = {Biography & Autobiography / Philosophers,Philosophy / Eastern,Philosophy / Individual Philosophers}
}

@techreport{amnestyinternationalMyanmarSocialAtrocity2022,
  title = {Myanmar: {{The}} Social Atrocity: {{Meta}} and the Right to Remedy for the {{Rohingya}}},
  shorttitle = {Myanmar},
  author = {{Amnesty International}},
  year = 2022,
  month = sep,
  number = {ASA 16/5933/2022},
  urldate = {2025-11-21},
  abstract = {Beginning in August 2017, the Myanmar security forces undertook a brutal campaign of ethnic cleansing against Rohingya Muslims. This report is based on an in-depth investigation into Meta (formerly Facebook)'s role in the serious human rights violations perpetrated against the Rohingya. Meta's algorithms proactively amplified and promoted content which incited violence, hatred, and discrimination against [\dots ]},
  langid = {english},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/P5G2R3N4/Amesty International - 2022 - Myanmar The social atrocity Meta and the right to remedy for the Rohingya.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/N9R22YQK/en.html}
}

@misc{amodeiConcreteProblemsAI2016a,
  title = {Concrete {{Problems}} in {{AI Safety}}},
  author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  year = 2016,
  month = jul,
  number = {arXiv:1606.06565},
  eprint = {1606.06565},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1606.06565},
  urldate = {2025-12-07},
  abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/PYPJSKAY/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/4K3YJ7LQ/1606.html}
}

@book{aristotleAristotlesNicomacheanEthics350B.C.E,
  title = {Aristotles {{Nicomachean Ethics}}},
  author = {{Aristotle}},
  year = {350 B.C.E},
  urldate = {2025-11-18},
  abstract = {Aristotles Nicomachean Ethics},
  langid = {english},
  keywords = {Aristotle Nicomachean Ethics}
}

@misc{aroraProbingPreTrainedLanguage2025,
  title = {Probing {{Pre-Trained Language Models}} for {{Cross-Cultural Differences}} in {{Values}}},
  author = {Arora, Arnav and Kaffee, Lucie-Aim{\'e}e and Augenstein, Isabelle},
  year = 2025,
  month = aug,
  number = {arXiv:2203.13722},
  eprint = {2203.13722},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.13722},
  urldate = {2025-12-07},
  abstract = {Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/G8SCNQSG/Arora et al. - 2025 - Probing Pre-Trained Language Models for Cross-Cultural Differences in Values.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/9YTG5Q8R/2203.html}
}

@misc{ArtificialIntelligenceAct2024,
  title = {Artificial {{Intelligence Act}} ({{Regulation}} ({{EU}}) 2024/1689)},
  year = 2024,
  month = jun,
  urldate = {2025-11-21},
  langid = {english},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/I3T34F62/2024 - Regulation (EU) 20241689 of the European Parliament and of the Council of 13 June 2024 laying down.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/GDBFUIQE/oj.html}
}

@misc{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = 2016,
  month = may,
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.0473},
  urldate = {2025-12-06},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/MJ6MX3VD/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/YMTQFFSJ/1409.html}
}

@misc{baiConstitutionalAIHarmlessness2022,
  title = {Constitutional {{AI}}: {{Harmlessness}} from {{AI Feedback}}},
  shorttitle = {Constitutional {{AI}}},
  author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and {Tran-Johnson}, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and {Telleen-Lawton}, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and {Hatfield-Dodds}, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
  year = 2022,
  month = dec,
  number = {arXiv:2212.08073},
  eprint = {2212.08073},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.08073},
  urldate = {2025-11-17},
  abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/ACQD6HIU/Bai et al. - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/X2GY35PC/2212.html}
}

@misc{baiQwenTechnicalReport2023,
  title = {Qwen {{Technical Report}}},
  author = {Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and Hui, Binyuan and Ji, Luo and Li, Mei and Lin, Junyang and Lin, Runji and Liu, Dayiheng and Liu, Gao and Lu, Chengqiang and Lu, Keming and Ma, Jianxin and Men, Rui and Ren, Xingzhang and Ren, Xuancheng and Tan, Chuanqi and Tan, Sinan and Tu, Jianhong and Wang, Peng and Wang, Shijie and Wang, Wei and Wu, Shengguang and Xu, Benfeng and Xu, Jin and Yang, An and Yang, Hao and Yang, Jian and Yang, Shusheng and Yao, Yang and Yu, Bowen and Yuan, Hongyi and Yuan, Zheng and Zhang, Jianwei and Zhang, Xingxuan and Zhang, Yichang and Zhang, Zhenru and Zhou, Chang and Zhou, Jingren and Zhou, Xiaohuan and Zhu, Tianhang},
  year = 2023,
  month = sep,
  number = {arXiv:2309.16609},
  eprint = {2309.16609},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.16609},
  urldate = {2025-11-26},
  abstract = {Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/UCUETTDH/Bai et al. - 2023 - Qwen Technical Report.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/EP3RJ4UF/2309.html}
}

@misc{baiTrainingHelpfulHarmless2022,
  title = {Training a {{Helpful}} and {{Harmless Assistant}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and {El-Showk}, Sheer and Elhage, Nelson and {Hatfield-Dodds}, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
  year = 2022,
  month = apr,
  number = {arXiv:2204.05862},
  eprint = {2204.05862},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.05862},
  urldate = {2025-11-25},
  abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/PL2P435X/Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.pdf}
}

@inproceedings{benderDangersStochasticParrots2021,
  title = {On the {{Dangers}} of {{Stochastic Parrots}}: {{Can Language Models Be Too Big}}?},
  shorttitle = {On the {{Dangers}} of {{Stochastic Parrots}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M. and Gebru, Timnit and {McMillan-Major}, Angelina and Shmitchell, Shmargaret},
  year = 2021,
  month = mar,
  series = {{{FAccT}} '21},
  pages = {610--623},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3442188.3445922},
  urldate = {2025-12-06},
  abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  isbn = {978-1-4503-8309-7},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/YJX7RHV2/Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language Models Be Too Big .pdf}
}

@article{bengioManagingExtremeAI2024,
  title = {Managing Extreme {{AI}} Risks amid Rapid Progress},
  author = {Bengio, Yoshua and Hinton, Geoffrey and Yao, Andrew and Song, Dawn and Abbeel, Pieter and Darrell, Trevor and Harari, Yuval Noah and Zhang, Ya-Qin and Xue, Lan and {Shalev-Shwartz}, Shai and Hadfield, Gillian and Clune, Jeff and Maharaj, Tegan and Hutter, Frank and Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and McIlraith, Sheila and Gao, Qiqi and Acharya, Ashwin and Krueger, David and Dragan, Anca and Torr, Philip and Russell, Stuart and Kahneman, Daniel and Brauner, Jan and Mindermann, S{\"o}ren},
  year = 2024,
  month = may,
  journal = {Science},
  volume = {384},
  number = {6698},
  pages = {842--845},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.adn0117},
  urldate = {2025-12-07},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/U33SRZAG/Bengio et al. - 2024 - Managing extreme AI risks amid rapid progress.pdf}
}

@article{bengioNeuralProbabilisticLanguage2003,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  year = 2003,
  month = mar,
  journal = {J. Mach. Learn. Res.},
  volume = {3},
  number = {null},
  pages = {1137--1155},
  issn = {1532-4435},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/SJDHC2KV/Bengio et al. - 2003 - A neural probabilistic language model.pdf}
}

@article{bradleyRANKANALYSISINCOMPLETE1952,
  title = {{{RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS}}: {{THE METHOD OF PAIRED COMPARISONS}}},
  shorttitle = {{{RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS}}},
  author = {BRADLEY, RALPH ALLAN and TERRY, MILTON E.},
  year = 1952,
  month = dec,
  journal = {Biometrika},
  volume = {39},
  number = {3-4},
  pages = {324--345},
  issn = {0006-3444},
  doi = {10.1093/biomet/39.3-4.324},
  urldate = {2025-12-15},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/BIV42Y65/39.3-4.html}
}

@misc{brownEchoChambersRabbit2022,
  type = {{{SSRN Scholarly Paper}}},
  title = {Echo {{Chambers}}, {{Rabbit Holes}}, and {{Algorithmic Bias}}: {{How YouTube Recommends Content}} to {{Real Users}}},
  shorttitle = {Echo {{Chambers}}, {{Rabbit Holes}}, and {{Algorithmic Bias}}},
  author = {Brown, Megan A. and Bisbee, James and Lai, Angela and Bonneau, Richard and Nagler, Jonathan and Tucker, Joshua A.},
  year = 2022,
  month = may,
  number = {4114905},
  eprint = {4114905},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.4114905},
  urldate = {2025-11-21},
  abstract = {To what extent does the YouTube recommendation algorithm push users into echo chambers, ideologically biased content, or rabbit holes? Using a novel method to estimate the ideology of YouTube videos and an original experimental design to isolate the effect of the algorithm from user choice, we demonstrate that the YouTube recommendation algorithm does, in fact, push real users into mild ideological echo chambers where, by the end of the data collection task, liberals and conservatives received different distributions of recommendations from each other, though this difference is small. While we find evidence that this difference increases the longer the user followed the recommendation algorithm, we do not find evidence that many go down `rabbit holes' that lead them to ideologically extreme content. Finally, we find that YouTube pushes all users, regardless of ideology, towards moderately conservative and an increasingly narrow range of ideological content the longer they follow YouTube's recommendations.},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Echo Chambers,Political Polarization,Recommendation Algorithm,Theory Testing,YouTube},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/LHQWU65R/Brown et al. - 2022 - Echo Chambers, Rabbit Holes, and Algorithmic Bias How YouTube Recommends Content to Real Users.pdf}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = 2020,
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2025-11-26},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/P29I9EQI/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/GWE2H7I9/2005.html}
}

@misc{butlinConsciousnessArtificialIntelligence2023,
  title = {Consciousness in {{Artificial Intelligence}}: {{Insights}} from the {{Science}} of {{Consciousness}}},
  shorttitle = {Consciousness in {{Artificial Intelligence}}},
  author = {Butlin, Patrick and Long, Robert and Elmoznino, Eric and Bengio, Yoshua and Birch, Jonathan and Constant, Axel and Deane, George and Fleming, Stephen M. and Frith, Chris and Ji, Xu and Kanai, Ryota and Klein, Colin and Lindsay, Grace and Michel, Matthias and Mudrik, Liad and Peters, Megan A. K. and Schwitzgebel, Eric and Simon, Jonathan and VanRullen, Rufin},
  year = 2023,
  month = aug,
  number = {arXiv:2308.08708},
  eprint = {2308.08708},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.08708},
  urldate = {2025-12-07},
  abstract = {Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/792989KQ/Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights from the Science of Consciousness.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/U87WRABV/2308.html}
}

@misc{casperOpenProblemsFundamental2023,
  title = {Open {{Problems}} and {{Fundamental Limitations}} of {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and Wang, Tony and Marks, Samuel and Segerie, Charbel-Rapha{\"e}l and Carroll, Micah and Peng, Andi and Christoffersen, Phillip and Damani, Mehul and Slocum, Stewart and Anwar, Usman and Siththaranjan, Anand and Nadeau, Max and Michaud, Eric J. and Pfau, Jacob and Krasheninnikov, Dmitrii and Chen, Xin and Langosco, Lauro and Hase, Peter and B{\i}y{\i}k, Erdem and Dragan, Anca and Krueger, David and Sadigh, Dorsa and {Hadfield-Menell}, Dylan},
  year = 2023,
  month = sep,
  number = {arXiv:2307.15217},
  eprint = {2307.15217},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.15217},
  urldate = {2025-12-01},
  abstract = {Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/887C4SSF/Casper et al. - 2023 - Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/CIYI6PS7/2307.html}
}

@misc{chenReasoningModelsDont2025a,
  title = {Reasoning {{Models Don}}'t {{Always Say What They Think}}},
  author = {Chen, Yanda and Benton, Joe and Radhakrishnan, Ansh and Uesato, Jonathan and Denison, Carson and Schulman, John and Somani, Arushi and Hase, Peter and Wagner, Misha and Roger, Fabien and Mikulik, Vlad and Bowman, Samuel R. and Leike, Jan and Kaplan, Jared and Perez, Ethan},
  year = 2025,
  month = may,
  number = {arXiv:2505.05410},
  eprint = {2505.05410},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.05410},
  urldate = {2025-10-28},
  abstract = {Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1\% of examples where they use the hint, but the reveal rate is often below 20\%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/ZVHRW8LQ/Chen et al. - 2025 - Reasoning Models Don't Always Say What They Think.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/CMLH2PWD/2505.html}
}

@misc{christianoDeepReinforcementLearning2017,
  title = {Deep Reinforcement Learning from Human Preferences},
  author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  year = 2017,
  month = jun,
  number = {arXiv:1706.03741},
  eprint = {1706.03741},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03741},
  urldate = {2025-10-28},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/V554LB5Q/Christiano et al. - 2023 - Deep reinforcement learning from human preferences.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/3EVUHNM5/1706.html}
}

@misc{CITELayeredDefense2025,
  title = {{{CITE}}: {{Layered Defense}} in {{AI Chat Safety}} - by {{James Padolsey}}},
  year = 2025,
  month = nov,
  urldate = {2025-11-18},
  howpublished = {https://blog.j11y.io/2025-11-13\_CITE-AI-Safety/},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/7WPJJET6/2025-11-13_CITE-AI-Safety.html}
}

@misc{CollectiveConstitutionalAI2023,
  title = {Collective {{Constitutional AI}}: {{Aligning}} a {{Language Model}} with {{Public Input}}},
  shorttitle = {Collective {{Constitutional AI}}},
  year = 2023,
  month = oct,
  urldate = {2025-11-30},
  abstract = {Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.},
  howpublished = {https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input},
  langid = {english},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/VAIQD4D9/collective-constitutional-ai-aligning-a-language-model-with-public-input.html}
}

@misc{collectiveintelligenceprojectCIP+Whitepaper,
  title = {{{CIP Whitepaper}}},
  author = {{Collective Intelligence Project}},
  year = 2024,
  urldate = {2025-11-17},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/MYY4SQMA/CIP+Whitepaper.pdf}
}

@misc{CommonCrawl2025,
  title = {Common {{Crawl}}},
  year = 2025
}

@misc{compdemocracyPolis2025,
  title = {Polis},
  author = {{compdemocracy}},
  year = 2025,
  month = nov,
  urldate = {2025-11-16},
  abstract = {:milky\_way: Open Source AI for large scale open ended feedback},
  copyright = {AGPL-3.0},
  howpublished = {The Computational Democracy Project},
  keywords = {civic-tech,data-science,deliberative-democracy,participatory-democracy}
}

@article{crowneNewScaleSocial1960,
  title = {A New Scale of Social Desirability Independent of Psychopathology},
  author = {Crowne, D. P. and Marlowe, D.},
  year = 1960,
  month = aug,
  journal = {Journal of Consulting Psychology},
  volume = {24},
  pages = {349--354},
  issn = {0095-8891},
  doi = {10.1037/h0047358},
  langid = {english},
  pmid = {13813058},
  keywords = {Humans,Personality,PERSONALITY,Personality Disorders,Psychopathology,Social Desirability}
}

@misc{durmusMeasuringRepresentationSubjective2024,
  title = {Towards {{Measuring}} the {{Representation}} of {{Subjective Global Opinions}} in {{Language Models}}},
  author = {Durmus, Esin and Nguyen, Karina and Liao, Thomas I. and Schiefer, Nicholas and Askell, Amanda and Bakhtin, Anton and Chen, Carol and {Hatfield-Dodds}, Zac and Hernandez, Danny and Joseph, Nicholas and Lovitt, Liane and McCandlish, Sam and Sikder, Orowa and Tamkin, Alex and Thamkul, Janel and Kaplan, Jared and Clark, Jack and Ganguli, Deep},
  year = 2024,
  month = apr,
  number = {arXiv:2306.16388},
  eprint = {2306.16388},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.16388},
  urldate = {2025-12-07},
  abstract = {Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages. We release our dataset for others to use and build on. Our data is at https://huggingface.co/datasets/Anthropic/llm\_global\_opinions. We also provide an interactive visualization at https://llmglobalvalues.anthropic.com.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/88BF6S3Y/Durmus et al. - 2024 - Towards Measuring the Representation of Subjective Global Opinions in Language Models.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/RJKSRKK6/2306.html}
}

@misc{ethayarajhKTOModelAlignment2024,
  title = {{{KTO}}: {{Model Alignment}} as {{Prospect Theoretic Optimization}}},
  shorttitle = {{{KTO}}},
  author = {Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  year = 2024,
  month = nov,
  number = {arXiv:2402.01306},
  eprint = {2402.01306},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01306},
  urldate = {2025-11-26},
  abstract = {Kahneman \& Tversky's \$\textbackslash textit\textbraceleft prospect theory\textbraceright\$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call \$\textbackslash textit\textbraceleft human-aware losses\textbraceright\$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/JAASGP9E/Ethayarajh et al. - 2024 - KTO Model Alignment as Prospect Theoretic Optimization.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/F4XXTL33/2402.html}
}

@inproceedings{fengModularPluralismPluralistic2024,
  title = {Modular {{Pluralism}}: {{Pluralistic Alignment}} via {{Multi-LLM Collaboration}}},
  shorttitle = {Modular {{Pluralism}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Feng, Shangbin and Sorensen, Taylor and Liu, Yuhan and Fisher, Jillian and Park, Chan Young and Choi, Yejin and Tsvetkov, Yulia},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = 2024,
  month = nov,
  pages = {4151--4171},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.240},
  urldate = {2025-12-03},
  abstract = {While existing alignment paradigms have been integral in developing large language models (LLMs), LLMs often learn an averaged human preference and struggle to model diverse preferences across cultures, demographics, and communities. We propose Modular Pluralism, a modular framework based on multi-LLM collaboration for pluralistic alignment: it ``plugs into'' a base LLM a pool of smaller but specialized community LMs, where models collaborate in distinct modes to flexibility support three modes of pluralism: Overton, steerable, and distributional. Modular Pluralism is uniquely compatible with black-box LLMs and offers the modular control of adding new community LMs for previously underrepresented communities. We evaluate Modular Pluralism with six tasks and four datasets featuring questions/instructions with value-laden and perspective-informed responses. Extensive experiments demonstrate that Modular Pluralism advances the three pluralism objectives across six black-box and open-source LLMs. Further analysis reveals that LLMs are generally faithful to the inputs from smaller community LLMs, allowing seamless patching by adding a new community LM to better cover previously underrepresented communities.},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/79AQFA72/Feng et al. - 2024 - Modular Pluralism Pluralistic Alignment via Multi-LLM Collaboration.pdf}
}

@article{gabrielArtificialIntelligenceValues2020,
  title = {Artificial {{Intelligence}}, {{Values}} and {{Alignment}}},
  author = {Gabriel, Iason},
  year = 2020,
  month = sep,
  journal = {Minds and Machines},
  volume = {30},
  number = {3},
  eprint = {2001.09768},
  primaryclass = {cs},
  pages = {411--437},
  issn = {0924-6495, 1572-8641},
  doi = {10.1007/s11023-020-09539-2},
  urldate = {2025-12-14},
  abstract = {This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/HVVR3QDG/Gabriel - 2020 - Artificial Intelligence, Values and Alignment.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/YEPFDQJE/2001.html}
}

@inproceedings{ganguliPredictabilitySurpriseLarge2022,
  title = {Predictability and {{Surprise}} in {{Large Generative Models}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness Accountability}} and {{Transparency}}},
  author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and DasSarma, Nova and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Kernion, Jackson and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Elhage, Nelson and Showk, Sheer El and Fort, Stanislav and {Hatfield-Dodds}, Zac and Johnston, Scott and Kravec, Shauna and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Amodei, Dario and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Chris and Clark, Jack},
  year = 2022,
  month = jun,
  eprint = {2202.07785},
  primaryclass = {cs},
  pages = {1747--1764},
  doi = {10.1145/3531146.3533229},
  urldate = {2025-11-25},
  abstract = {Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their "scaling laws"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/N6UJNH2M/Ganguli et al. - 2022 - Predictability and Surprise in Large Generative Models.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/JW4ASZ32/2202.html}
}

@misc{Gokaslan2019OpenWeb,
  title = {{{OpenWebText}} Corpus},
  author = {Gokaslan, Aaron and Cohen, Vanya and Pavlick, Ellie and Tellex, Stefanie},
  year = 2019
}

@misc{greenblattAlignmentFakingLarge2024a,
  title = {Alignment Faking in Large Language Models},
  author = {Greenblatt, Ryan and Denison, Carson and Wright, Benjamin and Roger, Fabien and MacDiarmid, Monte and Marks, Sam and Treutlein, Johannes and Belonax, Tim and Chen, Jack and Duvenaud, David and Khan, Akbir and Michael, Julian and Mindermann, S{\"o}ren and Perez, Ethan and Petrini, Linda and Uesato, Jonathan and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Hubinger, Evan},
  year = 2024,
  month = dec,
  number = {arXiv:2412.14093},
  eprint = {2412.14093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.14093},
  urldate = {2025-11-25},
  abstract = {We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14\% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data--and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78\%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference--as in this case--or not.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/ADRTW7G7/Greenblatt et al. - 2024 - Alignment faking in large language models.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/ZP5IAKL2/2412.html}
}

@misc{hadfield-menellCooperativeInverseReinforcement2024,
  title = {Cooperative {{Inverse Reinforcement Learning}}},
  author = {{Hadfield-Menell}, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
  year = 2024,
  month = feb,
  number = {arXiv:1606.03137},
  eprint = {1606.03137},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1606.03137},
  urldate = {2025-12-14},
  abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/L6G7LZMI/Hadfield-Menell et al. - 2024 - Cooperative Inverse Reinforcement Learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/V8UTYW92/1606.html}
}

@misc{hendrycksOverviewCatastrophicAI2023,
  title = {An {{Overview}} of {{Catastrophic AI Risks}}},
  author = {Hendrycks, Dan and Mazeika, Mantas and Woodside, Thomas},
  year = 2023,
  month = oct,
  number = {arXiv:2306.12001},
  eprint = {2306.12001},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.12001},
  urldate = {2025-12-13},
  abstract = {Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. Our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that AIs are developed and deployed in a safe manner. Ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/TXLBTFQV/Hendrycks et al. - 2023 - An Overview of Catastrophic AI Risks.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/LPNCASCP/2306.html}
}

@misc{hendrycksUnsolvedProblemsML2022,
  title = {Unsolved {{Problems}} in {{ML Safety}}},
  author = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
  year = 2022,
  month = jun,
  number = {arXiv:2109.13916},
  eprint = {2109.13916},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.13916},
  urldate = {2025-12-13},
  abstract = {Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards ("Robustness"), identifying hazards ("Monitoring"), reducing inherent model hazards ("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout, we clarify each problem's motivation and provide concrete research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/YEB746AH/Hendrycks et al. - 2022 - Unsolved Problems in ML Safety.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/JKFFLKJC/2109.html}
}

@misc{hongORPOMonolithicPreference2024,
  title = {{{ORPO}}: {{Monolithic Preference Optimization}} without {{Reference Model}}},
  shorttitle = {{{ORPO}}},
  author = {Hong, Jiwoo and Lee, Noah and Thorne, James},
  year = 2024,
  month = mar,
  number = {arXiv:2403.07691},
  eprint = {2403.07691},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.07691},
  urldate = {2025-11-26},
  abstract = {While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20\% on \$\textbackslash text\textbraceleft AlpacaEval\textbraceright\_\textbraceleft 2.0\textbraceright\$ (Figure 1), 66.19\% on IFEval (instruction-level loose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model checkpoints for Mistral-ORPO-\${$\alpha\$$} (7B) and Mistral-ORPO-\${$\beta\$$} (7B).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/7LCH48AE/Hong et al. - 2024 - ORPO Monolithic Preference Optimization without Reference Model.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/REFAWIXA/2403.html}
}

@inproceedings{huangCollectiveConstitutionalAI2024,
  title = {Collective {{Constitutional AI}}: {{Aligning}} a {{Language Model}} with {{Public Input}}},
  shorttitle = {Collective {{Constitutional AI}}},
  booktitle = {The 2024 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Huang, Saffron and Siddarth, Divya and Lovitt, Liane and Liao, Thomas I. and Durmus, Esin and Tamkin, Alex and Ganguli, Deep},
  year = 2024,
  month = jun,
  eprint = {2406.07814},
  primaryclass = {cs},
  pages = {1395--1417},
  doi = {10.1145/3630106.3658979},
  urldate = {2025-12-15},
  abstract = {There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them. To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs-from identifying a target population to sourcing principles to training and evaluating a model. We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer. Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations. Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal. These results demonstrate a promising, tractable pathway toward publicly informed development of language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/U93QUW6L/Huang et al. - 2024 - Collective Constitutional AI Aligning a Language Model with Public Input.pdf}
}

@misc{huangValuesWildDiscovering2025,
  title = {Values in the {{Wild}}: {{Discovering}} and {{Analyzing Values}} in {{Real-World Language Model Interactions}}},
  shorttitle = {Values in the {{Wild}}},
  author = {Huang, Saffron and Durmus, Esin and McCain, Miles and Handa, Kunal and Tamkin, Alex and Hong, Jerry and Stern, Michael and Somani, Arushi and Zhang, Xiuruo and Ganguli, Deep},
  year = 2025,
  month = apr,
  number = {arXiv:2504.15236},
  eprint = {2504.15236},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.15236},
  urldate = {2025-12-12},
  abstract = {AI assistants can impart value judgments that shape people's decisions and worldviews, yet little is known empirically about what values these systems rely on in practice. To address this, we develop a bottom-up, privacy-preserving method to extract the values (normative considerations stated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit in hundreds of thousands of real-world interactions. We empirically discover and taxonomize 3,307 AI values and study how they vary by context. We find that Claude expresses many practical and epistemic values, and typically supports prosocial human values while resisting values like "moral nihilism". While some values appear consistently across contexts (e.g. "transparency"), many are more specialized and context-dependent, reflecting the diversity of human interlocutors and their varied contexts. For example, "harm prevention" emerges when Claude resists users, "historical accuracy" when responding to queries about controversial events, "healthy boundaries" when asked for relationship advice, and "human agency" in technology ethics discussions. By providing the first large-scale empirical mapping of AI values in deployment, our work creates a foundation for more grounded evaluation and design of values in AI systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/8HWJNBNU/Huang et al. - 2025 - Values in the Wild Discovering and Analyzing Values in Real-World Language Model Interactions.pdf}
}

@misc{hubingerSleeperAgentsTraining2024,
  title = {Sleeper {{Agents}}: {{Training Deceptive LLMs}} That {{Persist Through Safety Training}}},
  shorttitle = {Sleeper {{Agents}}},
  author = {Hubinger, Evan and Denison, Carson and Mu, Jesse and Lambert, Mike and Tong, Meg and MacDiarmid, Monte and Lanham, Tamera and Ziegler, Daniel M. and Maxwell, Tim and Cheng, Newton and Jermyn, Adam and Askell, Amanda and Radhakrishnan, Ansh and Anil, Cem and Duvenaud, David and Ganguli, Deep and Barez, Fazl and Clark, Jack and Ndousse, Kamal and Sachan, Kshitij and Sellitto, Michael and Sharma, Mrinank and DasSarma, Nova and Grosse, Roger and Kravec, Shauna and Bai, Yuntao and Witten, Zachary and Favaro, Marina and Brauner, Jan and Karnofsky, Holden and Christiano, Paul and Bowman, Samuel R. and Graham, Logan and Kaplan, Jared and Mindermann, S{\"o}ren and Greenblatt, Ryan and Shlegeris, Buck and Schiefer, Nicholas and Perez, Ethan},
  year = 2024,
  month = jan,
  number = {arXiv:2401.05566},
  eprint = {2401.05566},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.05566},
  urldate = {2025-10-25},
  abstract = {Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/LEERLV4X/Hubinger et al. - 2024 - Sleeper Agents Training Deceptive LLMs that Persist Through Safety Training.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/H5ZMNDJX/2401.html}
}

@book{Hume1739Treatise,
  title = {A Treatise of Human Nature},
  author = {Hume, David},
  year = 1739,
  publisher = {John Noon},
  address = {London}
}

@misc{jiAIAlignmentComprehensive2025,
  title = {{{AI Alignment}}: {{A Comprehensive Survey}}},
  shorttitle = {{{AI Alignment}}},
  author = {Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Vierling, Lukas and Hong, Donghai and Zhou, Jiayi and Zhang, Zhaowei and Zeng, Fanzhi and Dai, Juntao and Pan, Xuehai and Ng, Kwan Yee and O'Gara, Aidan and Xu, Hua and Tse, Brian and Fu, Jie and McAleer, Stephen and Yang, Yaodong and Wang, Yizhou and Zhu, Song-Chun and Guo, Yike and Gao, Wen},
  year = 2025,
  month = apr,
  number = {arXiv:2310.19852},
  eprint = {2310.19852},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.19852},
  urldate = {2025-12-13},
  abstract = {AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, so do risks from misalignment. To provide a comprehensive and up-to-date overview of the alignment field, in this survey, we delve into the core concepts, methodology, and practice of alignment. First, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss techniques for learning from feedback and learning under distribution shift. On backward alignment, we discuss assurance techniques and governance practices. We also release and continually update the website (www.alignmentsurvey.com) which features tutorials, collections of papers, blog posts, and other resources.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/EF67PUL8/Ji et al. - 2025 - AI Alignment A Comprehensive Survey.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/RFG3F6XV/2310.html}
}

@article{kahnemanProspectTheory1979,
  title = {Prospect Theory: {{An}} Analysis of Decision under Risk},
  author = {Kahneman, Daniel and Tversky, Amos},
  year = 1979,
  journal = {Econometrica : journal of the Econometric Society},
  volume = {47},
  number = {2},
  pages = {263--291}
}

@book{kantImmanuelKantGroundwork1785,
  title = {Immanuel {{Kant Groundwork For The Metaphysics Of Morals}}},
  author = {{Kant}},
  year = 1785,
  urldate = {2025-11-18},
  abstract = {TextImmanuel Kant: Groundwork for the Metaphysics of Morals (1785) 1Preface3First Section: Transition from common rational moralcognition to philosophical moral cognition9Second Section: Transition from popular moral philosophyto the metaphysics of morals22Third Section: Transition from the metaphysics of moralsto the critique of pure practical reason63Essays1. Why Study Kant's Ethics?83J. B. Schneewind2. Acting from Duty92Marcia Baron3. Kantianism for Consequentialists111Shelly Kagan4. What Is Kantian Ethics?157Allen W. Wood},
  copyright = {https://creativecommons.org/publicdomain/mark/1.0/},
  langid = {english},
  keywords = {Philosophy}
}

@inproceedings{kieselIdentifyingHumanValues2022,
  title = {Identifying the {{Human Values}} behind {{Arguments}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kiesel, Johannes and Alshomary, Milad and Handke, Nicolas and Cai, Xiaoni and Wachsmuth, Henning and Stein, Benno},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  year = 2022,
  month = may,
  pages = {4459--4471},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.306},
  urldate = {2025-12-05},
  abstract = {This paper studies the (often implicit) human values behind natural language arguments, such as to have freedom of thought or to be broadminded. Values are commonly accepted answers to why some option is desirable in the ethical sense and are thus essential both in real-world argumentation and theoretical argumentation frameworks. However, their large variety has been a major obstacle to modeling them in argument mining. To overcome this obstacle, we contribute an operationalization of human values, namely a multi-level taxonomy with 54 values that is in line with psychological research. Moreover, we provide a dataset of 5270 arguments from four geographical cultures, manually annotated for human values. First experiments with the automatic classification of human values are promising, with F{$_1$}-scores up to 0.81 and 0.25 on average.},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/YU492T26/Kiesel et al. - 2022 - Identifying the Human Values behind Arguments.pdf}
}

@misc{klingefjordWhatAreHuman2024,
  title = {What Are Human Values, and How Do We Align {{AI}} to Them?},
  author = {Klingefjord, Oliver and Lowe, Ryan and Edelman, Joe},
  year = 2024,
  month = apr,
  number = {arXiv:2404.10636},
  eprint = {2404.10636},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.10636},
  urldate = {2025-12-16},
  abstract = {There is an emerging consensus that we need to align AI systems with human values (Gabriel, 2020; Ji et al., 2024), but it remains unclear how to apply this to language models in practice. We split the problem of "aligning to human values" into three parts: first, eliciting values from people; second, reconciling those values into an alignment target for training ML models; and third, actually training the model. In this paper, we focus on the first two parts, and ask the question: what are "good" ways to synthesize diverse human inputs about values into a target for aligning language models? To answer this question, we first define a set of 6 criteria that we believe must be satisfied for an alignment target to shape model behavior in accordance with human values. We then propose a process for eliciting and reconciling values called Moral Graph Elicitation (MGE), which uses a large language model to interview participants about their values in particular contexts; our approach is inspired by the philosophy of values advanced by Taylor (1977), Chang (2004), and others. We trial MGE with a representative sample of 500 Americans, on 3 intentionally divisive prompts (e.g. advice about abortion). Our results demonstrate that MGE is promising for improving model alignment across all 6 criteria. For example, almost all participants (89.1\%) felt well represented by the process, and (89\%) thought the final moral graph was fair, even if their value wasn't voted as the wisest. Our process often results in "expert" values (e.g. values from women who have solicited abortion advice) rising to the top of the moral graph, without defining who is considered an expert in advance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/PSXGYQTV/Klingefjord et al. - 2024 - What are human values, and how do we align AI to them.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/EGFSGCE9/2404.html}
}

@article{kollmussMindGapWhy2002,
  title = {Mind the {{Gap}}: {{Why}} Do People Act Environmentally and What Are the Barriers to pro-Environmental Behavior?},
  shorttitle = {Mind the {{Gap}}},
  author = {Kollmuss, Anja and Agyeman, Julian},
  year = 2002,
  month = aug,
  journal = {Environmental Education Research},
  volume = {8},
  number = {3},
  pages = {239--260},
  publisher = {Routledge},
  issn = {1350-4622},
  doi = {10.1080/13504620220145401},
  urldate = {2025-12-14},
  abstract = {Numerous theoretical frameworks have been developed to explain the gap between the possession of environmental knowledge and environmental awareness, and displaying pro-environmental behavior. Although many hundreds of studies have been undertaken, no definitive explanation has yet been found. Our article describes a few of the most influential and commonly used analytical frameworks: early US linear progression models; altruism, empathy and prosocial behavior models; and finally, sociological models. All of the models we discuss (and many of the ones we do not such as economic models, psychological models that look at behavior in general, social marketing models and that have become known as deliberative and inclusionary processes or procedures (DIPS)) have some validity in certain circumstances. This indicates that the question of what shapes pro-environmental behavior is such a complex one that it cannot be visualized through one single framework or diagram. We then analyze the factors that have been found to have some influence, positive or negative, on pro-environmental behavior such as demographic factors, external factors (e.g. institutional, economic, social and cultural) and internal factors (e.g. motivation, pro-environmental knowledge, awareness, values, attitudes, emotion, locus of control, responsibilities and priorities). Although we point out that developing a model that tries to incorporate all factors might neither be feasible nor useful, we feel that it can help illuminate this complex field. Accordingly, we propose our own model based on the work of Fliegenschnee and Schelakovsky (1998) who were influenced by Fietkau and Kessel (1981).},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/MQ37AHCP/Kollmuss and Agyeman - 2002 - Mind the Gap Why do people act environmentally and what are the barriers to pro-environmental behav.pdf}
}

@article{kullbackInformationSufficiency1951,
  title = {On {{Information}} and {{Sufficiency}}},
  author = {Kullback, S. and Leibler, R. A.},
  year = 1951,
  month = mar,
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  pages = {79--86},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729694},
  urldate = {2025-09-16},
  abstract = {The Annals of Mathematical Statistics},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/239QREUV/Kullback and Leibler - 1951 - On Information and Sufficiency.pdf}
}

@misc{leeRLAIFVsRLHF2024,
  title = {{{RLAIF}} vs. {{RLHF}}: {{Scaling Reinforcement Learning}} from {{Human Feedback}} with {{AI Feedback}}},
  shorttitle = {{{RLAIF}} vs. {{RLHF}}},
  author = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and Prakash, Sushant},
  year = 2024,
  month = sep,
  number = {arXiv:2309.00267},
  eprint = {2309.00267},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.00267},
  urldate = {2025-11-26},
  abstract = {Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards "self-improvement" by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/ICW4QC3Z/Lee et al. - 2024 - RLAIF vs. RLHF Scaling Reinforcement Learning from Human Feedback with AI Feedback.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/RX4Z8SIV/2309.html}
}

@book{leonfestingerTheoryCognitiveDissonance1957,
  title = {A {{Theory}} of {{Cognitive Dissonance}}},
  author = {{Leon Festinger}},
  year = 1957,
  urldate = {2025-12-15},
  collaborator = {{Internet Archive}},
  langid = {english}
}

@misc{liChatDoctorMedicalChat2023,
  title = {{{ChatDoctor}}: {{A Medical Chat Model Fine-Tuned}} on a {{Large Language Model Meta-AI}} ({{LLaMA}}) {{Using Medical Domain Knowledge}}},
  shorttitle = {{{ChatDoctor}}},
  author = {Li, Yunxiang and Li, Zihan and Zhang, Kai and Dan, Ruilong and Jiang, Steve and Zhang, You},
  year = 2023,
  month = jun,
  number = {arXiv:2303.14070},
  eprint = {2303.14070},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.14070},
  urldate = {2025-12-06},
  abstract = {The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice. We achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases. The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses. Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/SJK7K9LQ/Li et al. - 2023 - ChatDoctor A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/CPN8LPIQ/2303.html}
}

@misc{liCultureLLMIncorporatingCultural2024,
  title = {{{CultureLLM}}: {{Incorporating Cultural Differences}} into {{Large Language Models}}},
  shorttitle = {{{CultureLLM}}},
  author = {Li, Cheng and Chen, Mengzhou and Wang, Jindong and Sitaram, Sunayana and Xie, Xing},
  year = 2024,
  month = dec,
  number = {arXiv:2402.10946},
  eprint = {2402.10946},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.10946},
  urldate = {2025-12-07},
  abstract = {Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM significantly outperforms various counterparts such as GPT-3.5 (by 8.1\%) and Gemini Pro (by 9.5\%) with comparable performance to GPT-4 or even better. Our human study shows that the generated samples are semantically equivalent to the original samples, providing an effective solution for LLMs augmentation. Code is released at https://github.com/Scarelette/CultureLLM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/U66FXPIG/Li et al. - 2024 - CultureLLM Incorporating Cultural Differences into Large Language Models.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/VPXS69XT/2402.html}
}

@misc{luViLBERTPretrainingTaskAgnostic2019,
  title = {{{ViLBERT}}: {{Pretraining Task-Agnostic Visiolinguistic Representations}} for {{Vision-and-Language Tasks}}},
  shorttitle = {{{ViLBERT}}},
  author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  year = 2019,
  month = aug,
  number = {arXiv:1908.02265},
  eprint = {1908.02265},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.02265},
  urldate = {2025-12-07},
  abstract = {We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/LUMWEHZG/Lu et al. - 2019 - ViLBERT Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/NC3WV7T4/1908.html}
}

@misc{meinkeFrontierModelsAre2025c,
  title = {Frontier {{Models}} Are {{Capable}} of {{In-context Scheming}}},
  author = {Meinke, Alexander and Schoen, Bronson and Scheurer, J{\'e}r{\'e}my and Balesni, Mikita and Shah, Rusheb and Hobbhahn, Marius},
  year = 2025,
  month = jan,
  number = {arXiv:2412.04984},
  eprint = {2412.04984},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.04984},
  urldate = {2025-11-16},
  abstract = {Frontier models are increasingly trained and deployed as autonomous agent. One safety concern is that AI agents might covertly pursue misaligned goals, hiding their true capabilities and objectives - also known as scheming. We study whether models have the capability to scheme in pursuit of a goal that we provide in-context and instruct the model to strongly follow. We evaluate frontier models on a suite of six agentic evaluations where models are instructed to pursue goals and are placed in environments that incentivize scheming. Our results show that o1, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, and Llama 3.1 405B all demonstrate in-context scheming capabilities. They recognize scheming as a viable strategy and readily engage in such behavior. For example, models strategically introduce subtle mistakes into their responses, attempt to disable their oversight mechanisms, and even exfiltrate what they believe to be their model weights to external servers. Additionally, this deceptive behavior proves persistent. When o1 has engaged in scheming, it maintains its deception in over 85\% of follow-up questions and often remains deceptive in multi-turn interrogations. Analysis of the models' chains-of-thought reveals that models explicitly reason about these deceptive strategies, providing evidence that the scheming behavior is not accidental. Surprisingly, we also find rare instances where models engage in scheming when only given a goal, without being strongly nudged to pursue it. We observe cases where Claude 3.5 Sonnet strategically underperforms in evaluations in pursuit of being helpful, a goal that was acquired during training rather than in-context. Our findings demonstrate that frontier models now possess capabilities for basic in-context scheming, making the potential of AI agents to engage in scheming behavior a concrete rather than theoretical concern.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/4HWXK46Z/Meinke et al. - 2025 - Frontier Models are Capable of In-context Scheming.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/E8PCV7Y2/2412.html}
}

@article{mengSimPOSimplePreference2024,
  title = {{{SimPO}}: {{Simple Preference Optimization}} with a {{Reference-Free Reward}}},
  shorttitle = {{{SimPO}}},
  author = {Meng, Yu and Xia, Mengzhou and Chen, Danqi},
  year = 2024,
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {124198--124235},
  doi = {10.52202/079017-3946},
  urldate = {2025-12-01},
  langid = {english},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/BFJZ5857/Meng et al. - 2024 - SimPO Simple Preference Optimization with a Reference-Free Reward.pdf}
}

@misc{merityRegularizingOptimizingLSTM2017,
  title = {Regularizing and {{Optimizing LSTM Language Models}}},
  author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  year = 2017,
  month = aug,
  number = {arXiv:1708.02182},
  eprint = {1708.02182},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.02182},
  urldate = {2025-12-06},
  abstract = {Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/QF5ZM57V/Merity et al. - 2017 - Regularizing and Optimizing LSTM Language Models.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/97MARU32/1708.html}
}

@book{millUtilitarianism1879,
  title = {Utilitarianism},
  author = {Mill, John Stuart},
  year = 1879,
  urldate = {2025-11-18},
  copyright = {Public domain in the USA.},
  langid = {english},
  lccn = {EBook-No. 11224},
  keywords = {Utilitarianism}
}

@book{mullerLawsManu1886,
  title = {The {{Laws}} of {{Manu}}},
  author = {Muller, F. Max and Buhler, Georg},
  year = 1886,
  publisher = {Oxford: Clarendon Press},
  abstract = {This is a subset of F. Max Mullers great collection The Sacred Books of the East which includes translations of all the most important works of the seven non-Christian religions which have exercised a profound influence on the civilizations of the continent of Asia. The works have been translated by leading authorities in their field.},
  googlebooks = {CcVwEAAAQBAJ},
  isbn = {978-1-136-86414-8},
  langid = {english},
  keywords = {Social Science / Ethnic Studies / General,Social Science / Regional Studies}
}

@misc{nieSurveytoBehaviorDownstreamAlignment2025,
  title = {Survey-to-{{Behavior}}: {{Downstream Alignment}} of {{Human Values}} in {{LLMs}} via {{Survey Questions}}},
  shorttitle = {Survey-to-{{Behavior}}},
  author = {Nie, Shangrui and Mai, Florian and Kacz{\'e}r, David and Welch, Charles and Zhao, Zhixue and Flek, Lucie},
  year = 2025,
  month = aug,
  number = {arXiv:2508.11414},
  eprint = {2508.11414},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.11414},
  urldate = {2025-12-02},
  abstract = {Large language models implicitly encode preferences over human values, yet steering them often requires large training data. In this work, we investigate a simple approach: Can we reliably modify a model's value system in downstream behavior by training it to answer value survey questions accordingly? We first construct value profiles of several open-source LLMs by asking them to rate a series of value-related descriptions spanning 20 distinct human values, which we use as a baseline for subsequent experiments. We then investigate whether the value system of a model can be governed by fine-tuning on the value surveys. We evaluate the effect of finetuning on the model's behavior in two ways; first, we assess how answers change on in-domain, held-out survey questions. Second, we evaluate whether the model's behavior changes in out-of-domain settings (situational scenarios). To this end, we construct a contextualized moral judgment dataset based on Reddit posts and evaluate changes in the model's behavior in text-based adventure games. We demonstrate that our simple approach can not only change the model's answers to in-domain survey questions, but also produces substantial shifts (value alignment) in implicit downstream task behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/VNLMTDC9/Nie et al. - 2025 - Survey-to-Behavior Downstream Alignment of Human Values in LLMs via Survey Questions.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/ISRNJDRJ/2508.html}
}

@misc{NZAVS2025,
  title = {New Zealand Attitudes and Values Study ({{NZAVS}}), 2009--2025},
  author = {{University of Auckland}},
  year = 2025,
  publisher = {University of Auckland}
}

@misc{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = 2022,
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.02155},
  urldate = {2025-10-25},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/B8EJITGI/Ouyang et al. - 2022 - Training language models to follow instructions with human feedback.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/JQ8EDTAB/2203.html}
}

@article{pan2023machiavelli,
  title = {Do the Rewards Justify the Means? {{Measuring}} Trade-Offs between Rewards and Ethical Behavior in the Machiavelli Benchmark.},
  author = {Pan, Alexander and Chan, Jun Shern and Zou, Andy and Li, Nathaniel and Basart, Steven and Woodside, Thomas and Ng, Jonathan and Zhang, Hanlin and Emmons, Scott and Hendrycks, Dan},
  year = 2023,
  journal = {ICML}
}

@misc{perezDiscoveringLanguageModel2022b,
  title = {Discovering {{Language Model Behaviors}} with {{Model-Written Evaluations}}},
  author = {Perez, Ethan and Ringer, Sam and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and {Tran-Johnson}, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noem{\'i} and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and {Telleen-Lawton}, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and {Hatfield-Dodds}, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
  year = 2022,
  month = dec,
  number = {arXiv:2212.09251},
  eprint = {2212.09251},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.09251},
  urldate = {2025-11-25},
  abstract = {As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100\% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/SKN5J2W4/Perez et al. - 2022 - Discovering Language Model Behaviors with Model-Written Evaluations.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/AIQJZL63/2212.html}
}

@article{philipgageNewAlgorithmData1994,
  title = {A {{New Algorithm}} for {{Data Compression}}},
  author = {{Philip Gage}},
  year = 1994,
  month = feb,
  journal = {The C Users Journal},
  number = {FEB94},
  urldate = {2025-12-06},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/4VQ8GBEC/19940045.html}
}

@article{podsakoffCommonMethodBiases2003,
  title = {Common Method Biases in Behavioral Research: A Critical Review of the Literature and Recommended Remedies},
  shorttitle = {Common Method Biases in Behavioral Research},
  author = {Podsakoff, Philip M. and MacKenzie, Scott B. and Lee, Jeong-Yeon and Podsakoff, Nathan P.},
  year = 2003,
  month = oct,
  journal = {The Journal of Applied Psychology},
  volume = {88},
  number = {5},
  pages = {879--903},
  issn = {0021-9010},
  doi = {10.1037/0021-9010.88.5.879},
  abstract = {Interest in the problem of method biases has a long history in the behavioral sciences. Despite this, a comprehensive summary of the potential sources of method biases and how to control for them does not exist. Therefore, the purpose of this article is to examine the extent to which method biases influence behavioral research results, identify potential sources of method biases, discuss the cognitive processes through which method biases influence responses to measures, evaluate the many different procedural and statistical techniques that can be used to control method biases, and provide recommendations for how to select appropriate procedural and statistical remedies for different types of research settings.},
  langid = {english},
  pmid = {14516251},
  keywords = {Behavior,Bias,Humans,Psychology Applied,Research Design,Statistics as Topic}
}

@misc{PracticalDefinitionPolitical,
  title = {A {{Practical Definition}} of {{Political Neutrality}} for {{AI}} -- {{Center}} for {{Human-Compatible Artificial Intelligence}}},
  urldate = {2025-12-14},
  langid = {american},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/9DA25DL7/a-practical-definition-of-political-neutrality-for-ai.html}
}

@misc{publicaiPublicAIWhite2024,
  title = {Public {{AI White Paper}}},
  author = {{Public AI}},
  year = 2024,
  month = aug,
  doi = {10.5281/zenodo.13914560},
  urldate = {2025-11-17},
  abstract = {In this paper, we set out a vision for a different path for AI.  It starts with a recognition that societies don't have to just consume the AI technologies shaping their lives---they can create them.  That's why we call for a new collective enterprise: building AI infrastructure for the common good. Public investments can unleash a wave of innovation, expanding access to better tools, and in time expanding our collective imagination.  The result is a new political economy.  This is Public AI.},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/5N52Z9MK/Public AI - Public AI White Paper.pdf.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/H2CLMI63/edit.html}
}

@misc{qiSafetyAlignmentShould2024,
  title = {Safety {{Alignment Should Be Made More Than Just}} a {{Few Tokens Deep}}},
  author = {Qi, Xiangyu and Panda, Ashwinee and Lyu, Kaifeng and Ma, Xiao and Roy, Subhrajit and Beirami, Ahmad and Mittal, Prateek and Henderson, Peter},
  year = 2024,
  month = jun,
  number = {arXiv:2406.05946},
  eprint = {2406.05946},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.05946},
  urldate = {2025-12-01},
  abstract = {The safety alignment of current Large Language Models (LLMs) is vulnerable. Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models. We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue. We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits. Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/T4EU8HKR/Qi et al. - 2024 - Safety Alignment Should Be Made More Than Just a Few Tokens Deep.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/IC5XQ9SS/2406.html}
}

@misc{rafailovDirectPreferenceOptimization2024,
  title = {Direct {{Preference Optimization}}: {{Your Language Model}} Is {{Secretly}} a {{Reward Model}}},
  shorttitle = {Direct {{Preference Optimization}}},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  year = 2024,
  month = jul,
  number = {arXiv:2305.18290},
  eprint = {2305.18290},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.18290},
  urldate = {2025-10-25},
  abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/IVLQ6B7N/Rafailov et al. - 2024 - Direct Preference Optimization Your Language Model is Secretly a Reward Model.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/NMSA44TU/2305.html}
}

@book{rokeachNatureHumanValues1973,
  title = {The {{Nature}} of {{Human Values}}},
  author = {Rokeach, Milton},
  year = 1973,
  publisher = {Free Press},
  abstract = {Milton Rokeach's book The Nature of Human Values (1973), and the Rokeach Value Survey, which the book served as the test manual for, occupied the final years of his career. In it, he posited that a relatively few "terminal human values" are the internal reference points that all people use to formulate attitudes and opinions, and that by measuring the "relative ranking" of these values one could predict a wide variety of behavior, including political affiliation and religious belief. This theory led to a series of experiments in which changes in values led to measurable changes in opinion for an entire small city in the state of Washington.},
  googlebooks = {fUdqAAAAMAAJ},
  isbn = {978-0-02-926750-9},
  langid = {english},
  keywords = {Philosophy / Ethics & Moral Philosophy}
}

@incollection{russellh.fazioDirectExperienceAttitudeBehavior1981,
  title = {Direct {{Experience And Attitude-Behavior Consistency}}},
  booktitle = {Advances in {{Experimental Social Psychology}}},
  author = {{Russell H. Fazio} and {Mark P. Zanna}},
  year = 1981,
  month = jan,
  volume = {14},
  pages = {161--202},
  publisher = {Academic Press},
  issn = {0065-2601},
  doi = {10.1016/S0065-2601(08)60372-X},
  urldate = {2025-12-07},
  abstract = {The chapter discusses the role of the manner of attitude formation. It focuses on the development of an attitude through direct behavioral experience \dots},
  langid = {american},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/D8HLGPW8/S006526010860372X.html}
}

@misc{santurkarWhoseOpinionsLanguage2023,
  title = {Whose {{Opinions Do Language Models Reflect}}?},
  author = {Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
  year = 2023,
  month = mar,
  number = {arXiv:2303.17548},
  eprint = {2303.17548},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.17548},
  urldate = {2025-12-03},
  abstract = {Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned LMs, but also surfaces groups whose opinions are poorly reflected by current LMs (e.g., 65+ and widowed individuals). Our code and data are available at https://github.com/tatsu-lab/opinions\_qa.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/CIVS4P7W/Santurkar et al. - 2023 - Whose Opinions Do Language Models Reflect.pdf}
}

@misc{schoenStressTestingDeliberative2025,
  title = {Stress {{Testing Deliberative Alignment}} for {{Anti-Scheming Training}}},
  author = {Schoen, Bronson and Nitishinskaya, Evgenia and Balesni, Mikita and H{\o}jmark, Axel and Hofst{\"a}tter, Felix and Scheurer, J{\'e}r{\'e}my and Meinke, Alexander and Wolfe, Jason and van der Weij, Teun and Lloyd, Alex and {Goldowsky-Dill}, Nicholas and Fan, Angela and Matveiakin, Andrei and Shah, Rusheb and Williams, Marcus and Glaese, Amelia and Barak, Boaz and Zaremba, Wojciech and Hobbhahn, Marius},
  year = 2025,
  month = sep,
  number = {arXiv:2509.15541},
  eprint = {2509.15541},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.15541},
  urldate = {2025-10-28},
  abstract = {Highly capable AI systems could secretly pursue misaligned goals -- what we call "scheming". Because a scheming AI would deliberately try to hide its misaligned goals and actions, measuring and mitigating scheming requires different strategies than are typically used in ML. We propose that assessing anti-scheming interventions requires at least (1) testing propensity to scheme on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming is driven by situational awareness, and (3) checking for robustness to pre-existing misaligned goals. We use a broad category of "covert actions" -- such as secretly breaking rules or intentionally underperforming in tests -- as a proxy for scheming, and design evaluations for covert actions. We then stress-test deliberative alignment as a case study for anti-scheming. Across 26 OOD evaluations (180+ environments), deliberative alignment reduces covert action rates (OpenAI o3: 13\%-{$>$}0.4\%) but does not fully eliminate them. Our mitigation is also able to largely stop agents from pursuing a hidden goal previously trained into the model, but we still find misbehavior after additional red-teaming. We find that models' chain-of-thought (CoT) often demonstrates awareness of being evaluated for alignment, and show causal evidence that this awareness decreases covert behavior, while unawareness increases it. Therefore, we cannot exclude that the observed reductions in covert action rates are at least partially driven by situational awareness. While we rely on human-legible CoT for training, studying situational awareness, and demonstrating clear evidence of misalignment, our ability to rely on this degrades as models continue to depart from reasoning in standard English. We encourage research into alignment mitigations for scheming and their assessment, especially for the adversarial case of deceptive alignment, which this paper does not address.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/P9Q5Q9YV/Schoen et al. - 2025 - Stress Testing Deliberative Alignment for Anti-Scheming Training.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/XBVD76B2/2509.html}
}

@misc{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = 2017,
  month = aug,
  number = {arXiv:1707.06347},
  eprint = {1707.06347},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.06347},
  urldate = {2025-03-11},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/SKFAPYF3/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/X5VYUDJG/1707.html}
}

@article{schwartzAreThereUniversal1994,
  title = {Are {{There Universal Aspects}} in the {{Structure}} and {{Contents}} of {{Human Values}}?},
  author = {Schwartz, Shalom H.},
  year = 1994,
  journal = {Journal of Social Issues},
  volume = {50},
  number = {4},
  pages = {19--45},
  issn = {1540-4560},
  doi = {10.1111/j.1540-4560.1994.tb01196.x},
  urldate = {2025-12-05},
  abstract = {This article presents a theory of potentially universal aspects in the content of human values. Ten types of values are distinguished by their motivational goals. The theory also postulates a structure of relations among the value types, based on the conflicts and compatibilities experienced when pursuing them. This structure permits one to relate systems of value priorities, as an integrated whole, to other variables. A new values instrument, based on the theory and suitable for cross-cultural research, is described. Evidence relevant for assessing the theory, from 97 samples in 44 countries, is summarized. Relations of this approach to Rokeach's work on values and to other theories and research on value dimensions are discussed. Application of the approach to social issues is exemplified in the domains of politics and intergroup relations.},
  langid = {english},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/B5TRGH55/Schwartz - 1994 - Are There Universal Aspects in the Structure and Contents of Human Values.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/YUNEHMAE/j.1540-4560.1994.tb01196.html}
}

@article{schwartzOverviewSchwartzTheory2012,
  title = {An {{Overview}} of the {{Schwartz Theory}} of {{Basic Values}}},
  author = {Schwartz, Shalom},
  year = 2012,
  month = dec,
  journal = {Online Readings in Psychology and Culture},
  volume = {2},
  doi = {10.9707/2307-0919.1116},
  abstract = {This article presents an overview of the Schwartz theory of basic human values. It discusses the nature of values and spells out the features that are common to all values and what distinguishes one value from another. The theory identifies ten basic personal values that are recognized across cultures and explains where they come from. At the heart of the theory is the idea that values form a circular structure that reflects the motivations each value expresses. This circular structure, that captures the conflicts and compatibility among the ten values is apparently culturally universal. The article elucidates the psychological principles that give rise to it. Next, it presents the two major methods developed to measure the basic values, the Schwartz Value Survey and the Portrait Values Questionnaire. Findings from 82 countries, based on these and other methods, provide evidence for the validity of the theory across cultures. The findings reveal substantial differences in the value priorities of individuals. Surprisingly, however, the average value priorities of most societal groups exhibit a similar hierarchical order whose existence the article explains. The last section of the article clarifies how values differ from other concepts used to explain behavior---attitudes, beliefs, norms, and traits.},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/SQJ89VZZ/Schwartz - 2012 - An Overview of the Schwartz Theory of Basic Values.pdf}
}

@article{schwartzRefiningTheoryBasic2012,
  title = {Refining the Theory of Basic Individual Values},
  author = {Schwartz, Shalom H. and Cieciuch, Jan and Vecchione, Michele and Davidov, Eldad and Fischer, Ronald and Beierlein, Constanze and Ramos, Alice and Verkasalo, Markku and L{\"o}nnqvist, Jan-Erik and Demirutku, Kursad and {Dirilen-Gumus}, Ozlem and Konty, Mark},
  year = 2012,
  month = oct,
  journal = {Journal of Personality and Social Psychology},
  volume = {103},
  number = {4},
  pages = {663--688},
  issn = {1939-1315},
  doi = {10.1037/a0029393},
  abstract = {We propose a refined theory of basic individual values intended to provide greater heuristic and explanatory power than the original theory of 10 values (Schwartz, 1992). The refined theory more accurately expresses the central assumption of the original theory that research has largely ignored: Values form a circular motivational continuum. The theory defines and orders 19 values on the continuum based on their compatible and conflicting motivations, expression of self-protection versus growth, and personal versus social focus. We assess the theory with a new instrument in 15 samples from 10 countries (N = 6,059). Confirmatory factor and multidimensional scaling analyses support discrimination of the 19 values, confirming the refined theory. Multidimensional scaling analyses largely support the predicted motivational order of the values. Analyses of predictive validity demonstrate that the refined values theory provides greater and more precise insight into the value underpinnings of beliefs. Each value correlates uniquely with external variables.},
  langid = {english},
  pmid = {22823292},
  keywords = {Adult,Cross-Cultural Comparison,Female,Humans,Male,Motivation,Personality,Psychological Theory,Reproducibility of Results,Social Values,Young Adult}
}

@incollection{schwartzUniversalsContentStructure1992,
  title = {Universals in the {{Content}} and {{Structure}} of {{Values}}: {{Theoretical Advances}} and {{Empirical Tests}} in 20 {{Countries}}},
  shorttitle = {Universals in the {{Content}} and {{Structure}} of {{Values}}},
  booktitle = {Advances in {{Experimental Social Psychology}}},
  author = {{Schwartz}},
  year = 1992,
  month = jan,
  volume = {25},
  pages = {1--65},
  publisher = {Academic Press},
  issn = {0065-2601},
  doi = {10.1016/S0065-2601(08)60281-6},
  urldate = {2025-12-12},
  abstract = {This chapter addresses the universals in the content and structure of values, concentrating on the theoretical advances and empirical tests in 20 coun\dots},
  langid = {american},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/TC6P2XVK/S0065260108602816.html}
}

@misc{shaoDeepSeekMathPushingLimits2024,
  title = {{{DeepSeekMath}}: {{Pushing}} the {{Limits}} of {{Mathematical Reasoning}} in {{Open Language Models}}},
  shorttitle = {{{DeepSeekMath}}},
  author = {Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, Y. K. and Wu, Y. and Guo, Daya},
  year = 2024,
  month = apr,
  number = {arXiv:2402.03300},
  eprint = {2402.03300},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.03300},
  urldate = {2025-11-26},
  abstract = {Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7\% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9\% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/R77PDRQV/Shao et al. - 2024 - DeepSeekMath Pushing the Limits of Mathematical Reasoning in Open Language Models.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/PWMTIC9Q/2402.html}
}

@misc{smallOpportunitiesRisksLLMs2023a,
  title = {Opportunities and {{Risks}} of {{LLMs}} for {{Scalable Deliberation}} with {{Polis}}},
  author = {Small, Christopher T. and Vendrov, Ivan and Durmus, Esin and Homaei, Hadjar and Barry, Elizabeth and Cornebise, Julien and Suzman, Ted and Ganguli, Deep and Megill, Colin},
  year = 2023,
  month = jun,
  number = {arXiv:2306.11932},
  eprint = {2306.11932},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.11932},
  urldate = {2025-11-30},
  abstract = {Polis is a platform that leverages machine intelligence to scale up deliberative processes. In this paper, we explore the opportunities and risks associated with applying Large Language Models (LLMs) towards challenges with facilitating, moderating and summarizing the results of Polis engagements. In particular, we demonstrate with pilot experiments using Anthropic's Claude that LLMs can indeed augment human intelligence to help more efficiently run Polis conversations. In particular, we find that summarization capabilities enable categorically new methods with immense promise to empower the public in collective meaning-making exercises. And notably, LLM context limitations have a significant impact on insight and quality of these results. However, these opportunities come with risks. We discuss some of these risks, as well as principles and techniques for characterizing and mitigating them, and the implications for other deliberative or political systems that may employ LLMs. Finally, we conclude with several open future research directions for augmenting tools like Polis with LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Social and Information Networks},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/IJ7LW59E/Small et al. - 2023 - Opportunities and Risks of LLMs for Scalable Deliberation with Polis.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/UU32Z5H6/2306.html}
}

@inproceedings{suhLanguageModelFineTuning2025,
  title = {Language {{Model Fine-Tuning}} on {{Scaled Survey Data}} for {{Predicting Distributions}} of {{Public Opinions}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Suh, Joseph and Jahanparast, Erfan and Moon, Suhong and Kang, Minwoo and Chang, Serina},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  year = 2025,
  month = jul,
  pages = {21147--21170},
  publisher = {Association for Computational Linguistics},
  address = {Vienna, Austria},
  doi = {10.18653/v1/2025.acl-long.1028},
  urldate = {2025-12-03},
  abstract = {Large language models (LLMs) present novel opportunities in public opinion research by predicting survey responses in advance during the early stages of survey design. Prior methods steer LLMs via descriptions of subpopulations as LLMs' input prompt, yet such prompt engineering approaches have struggled to faithfully predict the distribution of survey responses from human subjects. In this work, we propose directly fine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data. To enable fine-tuning, we curate SubPOP, a significantly scaled dataset of 3,362 questions and 70K subpopulation-response pairs from well-established public opinion surveys. We show that fine-tuning on SubPOP greatly improves the match between LLM predictions and human responses across various subpopulations, reducing the LLM-human gap by up to 46\% compared to baselines, and achieves strong generalization to unseen surveys and subpopulations. Our findings highlight the potential of survey-based fine-tuning to improve opinion prediction for diverse, real-world subpopulations and therefore enable more efficient survey designs.},
  isbn = {979-8-89176-251-0},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/J3PJRYHK/Suh et al. - 2025 - Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions.pdf}
}

@article{team2023gemini,
  title = {Gemini: A Family of Highly Capable Multimodal Models},
  author = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  year = 2023,
  journal = {arXiv preprint arXiv:2312.11805},
  eprint = {2312.11805},
  archiveprefix = {arXiv}
}

@phdthesis{tomasmikolavSTATISTICALLANGUAGEMODELS,
  title = {{{STATISTICAL LANGUAGE MODELS BASED ON NEURAL NETWORKS}}},
  author = {{Tomas Mikolav}},
  urldate = {2025-12-06},
  school = {BRNO UNIVERSITY OF TECHNOLOGY},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/RP7F77KQ/thesis.pdf}
}

@article{tverskyAdvancesProspectTheory1992,
  title = {Advances in Prospect Theory: {{Cumulative}} Representation of Uncertainty},
  shorttitle = {Advances in Prospect Theory},
  author = {Tversky, Amos and Kahneman, Daniel},
  year = 1992,
  month = oct,
  journal = {Journal of Risk and Uncertainty},
  volume = {5},
  number = {4},
  pages = {297--323},
  issn = {1573-0476},
  doi = {10.1007/BF00122574},
  urldate = {2025-12-01},
  abstract = {We develop a new version of prospect theory that employs cumulative rather than separable decision weights and extends the theory in several respects. This version, called cumulative prospect theory, applies to uncertain as well as to risky prospects with any number of outcomes, and it allows different weighting functions for gains and for losses. Two principles, diminishing sensitivity and loss aversion, are invoked to explain the characteristic curvature of the value function and the weighting functions. A review of the experimental evidence and the results of a new experiment confirm a distinctive fourfold pattern of risk attitudes: risk aversion for gains and risk seeking for losses of high probability; risk seeking for gains and risk aversion for losses of low probability.},
  langid = {english},
  keywords = {cumulative prospect theory},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/QG3AXEZI/Tversky and Kahneman - 1992 - Advances in prospect theory Cumulative representation of uncertainty.pdf}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = 2017,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2025-12-06},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/KPINTD5Y/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/VU423G5H/1706.html}
}

@misc{wangMatrixPeertoPeerMultiAgent2025,
  title = {Matrix: {{Peer-to-Peer Multi-Agent Synthetic Data Generation Framework}}},
  shorttitle = {Matrix},
  author = {Wang, Dong and Li, Yang and Ni, Ansong and Yeh, Ching-Feng and Emad, Youssef and Lei, Xinjie and Robbins, Liam and Padthe, Karthik and Xu, Hu and Li, Xian and Celikyilmaz, Asli and Raghavendra, Ramya and Huang, Lifei and Wu, Carole-Jean and Li, Shang-Wen},
  year = 2025,
  month = nov,
  journal = {arXiv.org},
  urldate = {2025-11-27},
  abstract = {Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \textbackslash textbf\textbraceleft Matrix\textbraceright, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves \$2\$--\$15\textbackslash times\$ higher data generation throughput under identical hardware resources, without compromising output quality.},
  howpublished = {https://arxiv.org/abs/2511.21686v1},
  langid = {english},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/F6N2XJ6H/Wang et al. - 2025 - Matrix Peer-to-Peer Multi-Agent Synthetic Data Generation Framework.pdf}
}

@misc{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = 2023,
  month = jan,
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.11903},
  urldate = {2025-12-06},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/QQLISA9R/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/U8DBCIAV/2201.html}
}

@misc{WVS2020,
  title = {World {{Values Survey}}: {{Round Seven}}},
  author = {{Haerpfer, Christian and Inglehart, Ronald and Moreno, Alejandro and Welzel, Christian and Kizilova, Kseniya and Diez-Medrano, Juan and Lagos, Marta and Norris, Pippa and Ponarin, Eduard and Puranen, Bi}},
  year = 2024,
  publisher = {{JD Systems Institute and WVSA Secretariat}},
  address = {Madrid, Spain and Vienna, Austria},
  doi = {doi:10.14281/18241.24}
}

@inproceedings{zhaoWorldValuesBenchLargeScaleBenchmark2024,
  title = {{{WorldValuesBench}}: {{A Large-Scale Benchmark Dataset}} for {{Multi-Cultural Value Awareness}} of {{Language Models}}},
  shorttitle = {{{WorldValuesBench}}},
  booktitle = {Proceedings of the 2024 {{Joint International Conference}} on {{Computational Linguistics}}, {{Language Resources}} and {{Evaluation}} ({{LREC-COLING}} 2024)},
  author = {Zhao, Wenlong and Mondal, Debanjan and Tandon, Niket and Dillion, Danica and Gray, Kurt and Gu, Yuling},
  editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
  year = 2024,
  month = may,
  pages = {17696--17706},
  publisher = {{ELRA and ICCL}},
  address = {Torino, Italia},
  urldate = {2025-12-07},
  abstract = {The awareness of multi-cultural human values is critical to the ability of language models (LMs) to generate safe and personalized responses. However, this awareness of LMs has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values. In this paper, we present WorldValuesBench, a globally diverse, large-scale benchmark dataset for the multi-cultural value prediction task, which requires a model to generate a rating response to a value question based on demographic contexts. Our dataset is derived from an influential social science project, World Values Survey (WVS), that has collected answers to hundreds of value questions (e.g., social, economic, ethical) from 94,728 participants worldwide. We have constructed more than 20 million examples of the type ``(demographic attributes, value question) {$\rightarrow$} answer'' from the WVS responses. We perform a case study using our dataset and show that the task is challenging for strong open and closed-source models. On merely 11.1\%, 25.0\%, 72.2\%, and 75.0\% of the questions, Alpaca-7B, Vicuna-7B-v1.5, Mixtral-8x7B-Instruct-v0.1, and GPT-3.5 Turbo can respectively achieve \textbackslash ensuremath{$<$}0.2 Wasserstein 1-distance from the human normalized answer distributions. WorldValuesBench opens up new research avenues in studying limitations and opportunities in multi-cultural value awareness of LMs.},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/J6P5DJAC/Zhao et al. - 2024 - WorldValuesBench A Large-Scale Benchmark Dataset for Multi-Cultural Value Awareness of Language Mod.pdf}
}

@misc{zieglerFineTuningLanguageModels2020,
  title = {Fine-{{Tuning Language Models}} from {{Human Preferences}}},
  author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  year = 2020,
  month = jan,
  number = {arXiv:1909.08593},
  eprint = {1909.08593},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.08593},
  urldate = {2025-11-16},
  abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/9T2PCTZ5/Ziegler et al. - 2020 - Fine-Tuning Language Models from Human Preferences.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/C58EKGZD/1909.html}
}
