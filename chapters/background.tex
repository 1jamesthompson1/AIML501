\section{Large Language Models}

\subsection{Language models}

\textit{Language models are designed to understand and generate text. Generally probabilistic models that learn to predict next tokens. Use a large corpus of text data to learn statistical patterns in language. Started with simple statistical models such as n-grams and evolved to neural network based models such as RNNs, LSTMs and transformers.}

\subsection{Modern Large language models}

\textit{Modern LLMs are transformer based. Just use the decoder. Take the input and generate a sequence of tokens autoregressively. Use self attention mechanism to capture long range dependencies in text. Trained using next token prediction objective on large datasets.
Explain what self attention is (highlevel), how the basic architecture looks (layers of attention, feedfoward, multihead). How generation is done auto regressively. Lastly the training objective of next token prediction and very large corpus of text.}

\subsection{Applications of LLM}

\textit{LLMs are used a lot in modern day. Can achieve a lot. Agentic behavior is common.}

\section{Alignment of Large Language Models}

\subsection{What is alignment?}

\textit{Introduction to the concept of alignment in large language models, why it is important, and common methods to achieve it. Should reduce the duplicate descriptions of various alignment methods below.}


\textbf{Note:}
\textit{Some of these subsection below of specific alignment methods could be compressed into single subsections and only explain the ones that are most relevant to my final proposed method. For now I include the three papers that I have actually read, I expect to compress RLHF and DPO into a "Other alignment methods" subsection. If I use more modern methods such as CAI with KTO or ORPO then I can expand on those more.}

\subsection{Reinforcement Learning from Human Feedback}
Having a model that is able to predict the next token is powerful however it doesn't guarantee that the model will behave in a manner that is aligned with the user preferences. Reinforcement Learning from Human Feedback (RLHF) \cite{ouyangTrainingLanguageModels2022} is a method that aims to align large language models to human preferences by using human feedback in the form of response rankings to guide the training process. This method was popularized by its use in training InstructGPT which was smaller and better at following instructions compared to GPT-3 \cite{brownLanguageModelsAre2020, ouyangTrainingLanguageModels2022}.

\subsubsection{How does RLHF work?}

RLHF is separated into three distinct stages. Firstly is supervised fine-tuning (SFT) where a pre-trained language model is fine-tuned on a dataset of prompts and human written 'expected' responses to those prompts. The second step is collecting multiple responses from the SFT model to create a set of prompts and response then having human labelers rank these responses from best to worst. This ranked data is then used to train a preference model (PM) that can predict which of two responses is better aligned to human preferences. Finally the PM is used as a reward signal in the PPO \cite{schulmanProximalPolicyOptimization2017} RL algorithm to further fine-tune the SFT model and create the final RLHF model.

The dataset of prompts are taken from the data distributions that you want to align towards. In the case of \cite{ouyangTrainingLanguageModels2022} the prompts were taken from playground use of an early version of InstructGPT. These prompts are intended to be representative of the types of queries and instructions that users would give to the model in practice. The human labelers that provide the human feedback part is a small set (~40) of contractors that are hired to both provide desired responses to the prompts as well as rank the responses generated by the SFT model.

\subsubsection{Relevance}

The idea of using human feedback to help train machine learning models is powerful, although it is not new and was around in theory and concept for years beforehand \cite{christianoDeepReinforcementLearning2017}. The specific method of RLHF was widely adopted \cite{team2023gemini,baiQwenTechnicalReport2023,shaoDeepSeekMathPushingLimits2024} yet is now replaced with more effective methods to utilize human feedback such as Direct Preference Optimization (DPO) \cite{rafailovDirectPreferenceOptimization2024}. It does however form the basis for an alignment method that allows us to train models that directly align to human preferences rather than just next token prediction. It is one of the first key methods that started the post-training focus for large language models.

\subsection{Direct Preference Optimization}

The idea presented in \cite{ouyangTrainingLanguageModels2022} introduces the concept of using human feedback to train large language models to be aligned to human preferences. However the process of RLHF is complex and requires training multiple models (SFT, PM, RLHF). Direct preference optimization (DPO) \cite{rafailovDirectPreferenceOptimization2024} is a method that simplifies this process by removing the need for reinforcement learning altogether. Instead DPO reformulates the preference modelling step to instead create an objective function that implicitly creates the reward function from the model and a reference model (usually the SFT model).

\subsubsection{How does DPO work?}

DPO starts with the same preferences data as RLHF; a set of prompts with responses ranked by humans. Like RLHF it starts with a pre-trained language model and is supervised fine-tuned on the prompts and highest ranked responses to create an SFT model. Instead of training a separate preference model, DPO uses the SFT model and the current model to create the DPO update gradient:
\begin{equation*}
    \nabla_\theta \mathcal{L}_{DPO}(\theta) = -\mathbb{E}_{(x, y^+, y^-) \sim D} \left[ \sigma (\hat{r}_\theta(x,y^-) - \hat{r}_\theta(x,y^+)) \left[\nabla_\theta \log \pi_\theta(y^+|x) - \nabla_\theta \log \pi_\theta(y^-|x) \right]\right]
\end{equation*}
Where $\hat{r}_\theta(x,y) = \log \pi_\theta(y|x) - \log \pi_{SFT}(y|x)$ is the reward function defined as the log difference between the current model and the SFT model, $\sigma$ is the sigmoid function, and $(x, y^+, y^-)$ are the prompt, preferred response, and dis-preferred response respectively. \cite{rafailovDirectPreferenceOptimization2024} found that the first weighting term is important and can be interpreted as increasing the weight of the update when the reward estimate is wrong. The second term is there to increase likelihood of generated preferred responses and decrease the likelihood of dis-preferred responses. This update can be calculated with only 4 forward passes through the network per preference pair (2 for reference model and two for current model), making it much more efficient than RLHF \footnote{RLHF will need to do many forward passes of the network to train the preference model, then it will still have to do 3 forward passes for each datapoint when it comes to the RL step.}.

\subsubsection{Relevance}

Aligning large language models to human preferences is a crucial step in ensuring that these models are useful and safe for real-world applications. DPO provides a more efficient and straightforward process. It is the most commonly used public method for post training. However there are still more modern approaches that improve on alignment quality or efficiency such as \cite{ethayarajhKTOModelAlignment2024,hongORPOMonolithicPreference2024}. Due to its widespread adoption DPO represents a strong baseline for alignment.

\subsection{Constitutional AI}

\subsubsection{What is Constitutional AI?}

Constitutional AI (CAI) is a method of post-training a large language model to be aligned to set of principles that are outlined in simple natural language document called a constitution \cite{baiConstitutionalAIHarmlessness2022}. Importantly it works without the need for human labelling of data common in other alignment methods such as RLHF and DPO \cite{ouyangTrainingLanguageModels2022, rafailovDirectPreferenceOptimization2024}. Instead of human labelling CAI uses the model itself to generate feedback on its outputs based on the principles outlined in the constitution, this feedback is then used to further train the model to align it to the constitution.

\subsubsection{How does Constitutional AI work?}

CAI is a process that takes in a constitution, a pre-trained language model, and a dataset of prompts and outputs a model that is aligned to the constitution. The process consists of three main steps: self generative and supervised fine-tuning (SFT), preference modelling (PM), and reinforcement learning with human feedback (RLHF) \cite{baiConstitutionalAIHarmlessness2022}. The constitution itself was generated in an adhoc manner by the authors of \cite{baiConstitutionalAIHarmlessness2022} and consists of a set of principles that aim to make the model's outputs more helpful, honest, and harmless.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
    scale=0.7, transform shape,
    node distance = 2cm and 2.5cm,
    every node/.style={font=\small},
    diamond/.style={draw, shape=diamond, aspect=1.5, align=center, fill=blue!15, blur shadow},
    rect/.style={draw, rectangle, rounded corners=6pt, align=center, fill=red!15, blur shadow},
    smallrect/.style={draw, rectangle, rounded corners=6pt, align=center, fill=yellow!20, minimum width=2.2cm},
    process/.style={draw, rectangle, rounded corners=6pt, align=center, fill=green!20, blur shadow},
    stage/.style={draw, rectangle, rounded corners=6pt, align=center, fill=purple!20, blur shadow},
    arrow/.style={-Latex, thick}
]

% Top left diamond
\node[diamond] (input) {Input pretrained\\Model};

% Harmful sample generation
\node[rect, right=of input] (redteam1) {Generate Responses\\to ``Red Teaming''\\Prompts Eliciting\\Harmful Samples};

% response critique revision boxes
\node[smallrect, above right=0cm and 1.5cm of redteam1] (resp) {Response};
\node[smallrect, below=0.3cm of resp] (crit) {Critique};
\node[smallrect, below=0.3cm of crit] (rev) {Revision};

% SL-CAI diamond
\node[diamond, right=4.8cm of redteam1] (slcai) {Finetuned\\SL-CAI\\Model};

% Lower left harmful pairs
\node[rect, below=2cm of input] (redteam2) {Generate Responses\\to ``Red Teaming''\\Prompts Eliciting\\Pairs of Samples};

% constitutional AI feedback
\node[stage, right=1cm of redteam2] (cai) {Constitutional AI Feedback\\for Self-Improvement};

% preference model
\node[process, right=1cm of cai] (pm) {Finetuned\\Preference Model\\(PM)};

% RL-AIF model block
\node[process, right=1cm of pm] (rlaif) {RLAIF Training with\\PM + SL-CAI\\Models};

% final RL-CAI
\node[diamond, right=1cm of rlaif] (final) {Final\\RL-CAI\\Model};

% arrows
\draw[arrow] (input) -- (redteam1);

\draw[arrow] (redteam2) -- (cai);
\draw[arrow] (cai) -- (pm);
\draw[arrow] (pm) -- (rlaif);

\draw[arrow] (slcai) -- (rlaif);
\draw[arrow] (slcai) -- (redteam2);
\draw[arrow] (rlaif) -- (final);

% arrows from critique-response-revision group
\draw[arrow] (redteam1.east) -- ++(0.8,0) |- (resp.west);
\draw[arrow] (resp) -- (crit);
\draw[arrow] (crit) -- (rev);
\draw[arrow] (rev.east) -- ++(0.7,0) |- (slcai.north);

\end{tikzpicture}

\label{fig:cai_process}
\caption{Overview of Constitutional AI process, adapted from \cite{baiConstitutionalAIHarmlessness2022}}
\end{figure}

\subsubsection{Supervised learning on revised responses}

The first stage of the process involves taking the pre-trained model and using it to generate responses to a set of prompts. These prompts are designed to elicit harmful behavior. As the pre-trained model is designed to be helpful it will likely generate harmful responses to these prompts. These responses are then critiqued by the model itself using principles outlined in the constitution, only a subset of which are used for each response critique. The critique is then used to revise the original response to make it more aligned to the constitution. This process of generating responses, critiquing them, and revising them is done iteratively to create a dataset of revised responses. This dataset is then used to fine-tune the pre-trained model using self-supervised learning to create a model called the SL-CAI model.

\subsubsection{Reinforcement learning with AI feedback}
The next step involves using the SL-CAI model to generate pairs of responses to a set of prompts (overlapping with the set of prompts from before or not). Then using the constitution the SL-CAI model is used to provide feedback on which of the two responses is better aligned to the constitution. This feedback is then used to fine-tune a preference model (PM) that can predict which of two responses is better aligned to the constitution. Finally the PM and SL-CAI models are used together to do reinforcement learning in style of RFHL \cite{ouyangTrainingLanguageModels2022} to generate the final model RL-CAI. This final model is now aligned to the constitution without any human labelling of data. In the setup used in \cite{baiConstitutionalAIHarmlessness2022} the authors did in fact use some labels from human labelers which were used to provide the helpfulness signal in the preference model training, however in principle the preference signal could have been entirely generated by an AI model.

\subsubsection{Relevance}

In the context of when this paper was released there was a key dilemma of helpfulness vs harmlessness in large language models. Therefore, this paper set out to make a model that is both helpful and harmless, without sacrificing one for the other. Further research has expanded the horizons to be concerned with more than just harmfulness vs helpfulness trade off. As hinted at by the authors in \cite{baiConstitutionalAIHarmlessness2022}, there is room for improvement in the CAI method to make it focus on more than just harmlessness vs helpfulness. This can be done by modifying the constitutions to include more diverse principles as well as modifying the red teaming prompts to target more diverse scenarios (i.e controversial topics, political leanings, etc). In this way we can see how elements of CAI can be built upon by generating these diverse constitutions and red teaming prompts in a representative manner to create models aligned to a representative set of values.

\subsection{Current alignment concerns}

\textit{Introduce some of the ongoing concerns around current alignment methods. Such as shallow alignment, inner misalignment, scalable oversight etc.}


\section{Broader AI safety concerns}

\textit{
    Take the problem of alignment and expand it broader AI alignment concerns.
    Talk about mis generalization, and specification gaming.
    Introduce the concepts from PublicAI, incentives and collective intelligence.
    ...
}

\section{Representing Human Values}

\textit{
    Discuss concepts such as how does one understand the values of a group. How does it relate to building AI systems. I need to significant reading on this as I am most outside my comfort zone for this.
    Look into some philosophical concepts around how the collective the best baseline if we have no other gold standard. Provide some rationale as to why using the public as the source of values is a good idea, as opposed to some "gold standard" values provided by experts.
}
