\section{Large Language Models}

\textit{A basic introduction to large language models that are transformer based. Including simple description of self attention, along with training process of pre-training and next token prediction.}

\section{Alignment of Large Language Models}

\subsection{What is alignment?}

\textit{Introduction to the conecpt of alignment in large language models, why it is important, and common methods to achieve it.}

\subsection{Reinforcement Learning from Human Feedback}
Having a model that is able to predict the next token is powerful however it doesn't guarantee that the model will behave in a manner that is aligned with the user preferences. Reinforcement Learning from Human Feedback (RLHF) \cite{ouyangTrainingLanguageModels2022} is a method that aims to align large language models to human preferences by using human feedback in the form of response rankings to guide the training process. This method was popularized by its use in training InstructGPT as giving it much better performance on following instructions compared to GPT-3 \cite{brownLanguageModelsAre2020, ouyangTrainingLanguageModels2022}.

\subsubsection{How does RLHF work?}

RLHF is separated into three distinct stages. Firstly is supervised fine-tuning (SFT) where a pre-trained language model is fine-tuned on a dataset of prompts and human written 'expected' responses to those prompts. The second step is collecting multiple responses from the SFT model to a set of prompts and having human labelers rank these responses from best to worst. This ranked data is then used to train a preference model (PM) that can predict which of two responses is better aligned to human preferences. Finally the PM is used as a reward signal in the \cite{schulmanProximalPolicyOptimization2017} RL algorithm to further fine-tune the SFT model and create the final RLHF model.

The dataset of prompts are taken from the data distributions that you want to align towards. In the case of \cite{ouyangTrainingLanguageModels2022} the prompts were taken from playground use of an early version of InstructGPT. These prompts are intended to be representative of the types of queries and instructions that users would give to the model in practice. The human labelers that provide the human feedback part is a small set (~40) of contractors that are hired to both provide desired responses to the prompts as well as rank the responses generated by the SFT model.

\subsubsection{Relevance}

The idea of using human feedback to help train machine learning models is powerful, although it is not new \cite{christianoDeepReinforcementLearning2017}. The specific method of  RLHF was adopted widely \cite{team2023gemini,baiQwenTechnicalReport2023,shaoDeepSeekMathPushingLimits2024} yet was replaced with more effective methods to utilize human feedback such as Direct Preference Optimization (DPO) \cite{rafailovDirectPreferenceOptimization2024}. It does however form the basis for an alignment method that allows us to train models that directly align to human preferences rather than just next token prediction. It is one of the first key methods that created the post training methods for large language models.

\subsection{Direct Preference Optimization}

The idea presented in \cite{ouyangTrainingLanguageModels2022} introduces the concept of using human feedback to train large language models to be aligned to human preferences. However the process of RLHF is complex and requires training multiple models (SFT, PM, RLHF). Direct preference optimization (DPO) \cite{rafailovDirectPreferenceOptimization2024} is a method that simplifies this process by removing the need for reinforcement learning altogether. Instead DPO reformulates the preference modelling step to instead create a preference model implicitly from the model itself.

\subsubsection{How does DPO work?}

DPO starts wtih the same preferences data as RLHF, a set of prompts with responses ranked by humans. Like RLHF it starts with a pre-trained language model and is supervised fine-tuned on the prompts and highest ranked responses to create an SFT model. Instead of training a separate preference model, DPO uses the SFT model itself to create the DPO update rule:
\begin{equation*}
    \nabla_\theta \mathcal{L}_{DPO}(\theta) = -\mathbb{E}_{(x, y^+, y^-) \sim D} \left[ \sigma (\hat{r}_\theta(x,y^-) - \hat{r}_\theta(x,y^+)) \left[\nabla_\theta \log \pi_\theta(y^+|x) - \nabla_\theta \log \pi_\theta(y^-|x) \right]\right]
\end{equation*}
Where $\hat{r}_\theta(x,y) = \log \pi_\theta(y|x) - \log \pi_{SFT}(y|x)$ is the reward function defined as the log difference between the current model and the SFT model, $\sigma$ is the sigmoid function, and $(x, y^+, y^-)$ are the prompt, preferred response, and dis-preferred response respectively. \cite{rafailovDirectPreferenceOptimization2024} found that the first weighting term is important and can be interpreted as increasing the weight of the update when the reward estimate is wrong. The second term is there to increase likelihood of generated preferred responses and decrease the likelihood of dis-preferred responses. This update can be calculated with only 4 forward passes through the network per preference pair, making it much more efficient than RLHF \footnote{RLHF will need to do many forward passes of the network to train the preference model, then it will still have to do 3 forward passes for each datapoint when it comes to the RL step.}.

\subsubsection{Relevance}

Aligning large language models to human preferences is a crucial step in ensuring that these models are useful and safe for real-world applications. DPO provides a more efficient and straight forward process. It is the most comonly used public methold for post training. However there are still more modern approaches that improve on alignment quality or efficiency such as \cite{ethayarajhKTOModelAlignment2024,hongORPOMonolithicPreference2024}. Due to its wide spread adoption DPO represents a strong baseline for alignment.

\subsection{Constitutional AI}

\subsubsection{What is Constitutional AI?}

Constitutional AI (CAI) is a method of post-training a large language model to be aligned to set of principles that are outlined in simple natural language document called a constitution \cite{baiConstitutionalAIHarmlessness2022}. Importantly it works without the need for human labelling of data common in other alignment methods such as RLHF and DPO \cite{ouyangTrainingLanguageModels2022, rafailovDirectPreferenceOptimization2024}. INstead of human labelling CAI uses the model itself to generate feedback on its outputs based on the principles outlined in the constitution, this feedback is then used to further train the model to align it to the constitution.


\subsubsection{How does Constitutional AI work?}

CAI is a process that takes in a constitution, a pre-trained language model, and a dataset of prompts and outputs a model that is aligned to the constitution. The process consists of three main steps: self generative and supervised fine-tuning (SFT), preference modelling (PM), and reinforcement learning with human feedback (RLHF) \cite{baiConstitutionalAIHarmlessness2022}. The constitution was generated in an adhoc manner by the authors of \cite{baiConstitutionalAIHarmlessness2022} and consists of a set of principles that aim to make the model's outputs more helpful, honest, and harmless.

\begin{figure}
\centering
\begin{tikzpicture}[
    scale=0.7, transform shape,
    node distance = 2cm and 2.5cm,
    every node/.style={font=\small},
    diamond/.style={draw, shape=diamond, aspect=1.5, align=center, fill=blue!15, blur shadow},
    rect/.style={draw, rectangle, rounded corners=6pt, align=center, fill=red!15, blur shadow},
    smallrect/.style={draw, rectangle, rounded corners=6pt, align=center, fill=yellow!20, minimum width=2.2cm},
    process/.style={draw, rectangle, rounded corners=6pt, align=center, fill=green!20, blur shadow},
    stage/.style={draw, rectangle, rounded corners=6pt, align=center, fill=purple!20, blur shadow},
    arrow/.style={-Latex, thick}
]

% Top left diamond
\node[diamond] (input) {Input pretrained\\Model};

% Harmful sample generation
\node[rect, right=of input] (redteam1) {Generate Responses\\to ``Red Teaming''\\Prompts Eliciting\\Harmful Samples};

% response critique revision boxes
\node[smallrect, above right=0cm and 1.5cm of redteam1] (resp) {Response};
\node[smallrect, below=0.3cm of resp] (crit) {Critique};
\node[smallrect, below=0.3cm of crit] (rev) {Revision};

% SL-CAI diamond
\node[diamond, right=4.8cm of redteam1] (slcai) {Finetuned\\SL-CAI\\Model};

% Lower left harmful pairs
\node[rect, below=2cm of input] (redteam2) {Generate Responses\\to ``Red Teaming''\\Prompts Eliciting\\Pairs of Samples};

% constitutional AI feedback
\node[stage, right=1cm of redteam2] (cai) {Constitutional AI Feedback\\for Self-Improvement};

% preference model
\node[process, right=1cm of cai] (pm) {Finetuned\\Preference Model\\(PM)};

% RL-AIF model block
\node[process, right=1cm of pm] (rlaif) {RLAIF Training with\\PM + SL-CAI\\Models};

% final RL-CAI
\node[diamond, right=1cm of rlaif] (final) {Final\\RL-CAI\\Model};

% arrows
\draw[arrow] (input) -- (redteam1);

\draw[arrow] (redteam2) -- (cai);
\draw[arrow] (cai) -- (pm);
\draw[arrow] (pm) -- (rlaif);

\draw[arrow] (slcai) -- (rlaif);
\draw[arrow] (slcai) -- (redteam2);
\draw[arrow] (rlaif) -- (final);

% arrows from critique-response-revision group
\draw[arrow] (redteam1.east) -- ++(0.8,0) |- (resp.west);
\draw[arrow] (resp) -- (crit);
\draw[arrow] (crit) -- (rev);
\draw[arrow] (rev.east) -- ++(0.7,0) |- (slcai.north);

\end{tikzpicture}

\label{fig:cai_process}
\caption{Overview of Constitutional AI process, adapted from \cite{baiConstitutionalAIHarmlessness2022}}
\end{figure}

\subsubsection{Supervised learning on revised responses}

The first stage of the process involves taking the pre-trained model and using it to generate responses to a set of prompts. These prompts are designed to elicit harmful behavior. As the pre-trained model is designed to be helpful it will likely generate harmful responses to these prompts. These responses are then critiqued by the model itself using principles outlined in the constitution, only a subset of which are used for each response. The critique is then used to revise the original response to make it more aligned to the constitution. This process of generating responses, critiquing them, and revising them is done iteratively to create a dataset of revised responses. This dataset is then used to fine-tune the pre-trained model using self-supervised learning to create a model called the SL-CAI model.

\subsubsection{Reinforcement learning with AI feedback}
The next step involves using the SL-CAI model to generate pairs of responses to a set of prompts (overlapping with before or not). Then using the constitution the SL-CAI model is used to provide feedback on which of the two responses is better aligned to the constitution. This feedback is then used to fine-tune a preference model (PM) that can predict which of two responses is better aligned to the constitution. Finally the PM and SL-CAI models are used together to do reinforcement learning in style of RFHL \cite{ouyangTrainingLanguageModels2022} to generate the final model RL-CAI. This final model is now aligned to the constitution without any human labelling of data. In the setup used in \cite{baiConstitutionalAIHarmlessness2022} the authors did in fact use some labels from human labelers which were used to provide the helpfulness signal in the preference model training, however in principle this step could of been done with AI.

\subsubsection{Relevance}

In the context of the time that this paper was to address the key dilemma of helpfulness vs harmlessness in large language models. The goal being to make a model that is both helpful and harmless, without sacrificing one for the other. Further research has expanded the horizons to be concerned with more than just harmfulness vs helpfulness trade off. Along with this there are concerns that methods like CAI and RLHF are simply superficially aligning models to avoid harmful outputs rather than deeply aligning to the values we want \cite{greenblattAlignmentFakingLarge2024a,hubingerSleeperAgentsTraining2024}. As hinted at by the authors in \cite{baiConstitutionalAIHarmlessness2022}, there is room for improvement in the CAI method to make it focus on more than just harmlessness vs helpfulness. This can be done by modifying the constitutions to include more diverse principles as well as modifying the red teaming prompts to target more diverse scenarios. In this way we can see how elements of CAI can be built upon by generating these diverse constitutions and red teaming prompts in a representative manner to create models aligned to a representative set of values.

\section{Representing Human Values}

\textit{Some some introduction to the problems of representing human values. How does one understand the values of a group and such. Furthermore relate it to how this ties into designing and building AI systems.}
