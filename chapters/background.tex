\section{Large Language Models}

\subsection{Language models}

Language models (LMs) are a class of machine learning models that are designed to understand and generate human language. They are typically probabilistic models that learn to predict the next token in a sequence of text given the preceding context. By training on large corpora of text data, language models can learn the statistical patterns and structures inherent in human language, enabling them to generate coherent and contextually relevant text.

Language models see the world as tokens which are the smallest pieces of text it can see. Tokens can represent words, parts of words, or even individual characters depending on the tokenization method used. Most language models today use Byte pair encoding \cite{philipgageNewAlgorithmData1994} that breaks text into subwords. Almost all successful language models use probabilistic methods, therefore they are models of the form $P(t_i|t_1, t_2, ..., t_{i-1})$ where $t_i$ is the token to be predicted and $t_1, t_2, ..., t_{i-1}$ are the preceding tokens in the sequence. Earlier methods of rule based systems failed due to the complexity and ambiguity of human language and rules being too rigid.

Early probabilistic models included simple statistical approaches such as n-grams, which relied on counting the frequency of token sequences. However this approach suffered from two competing issues, short context windows (2-grams, 3-grams etc) means that model can't understand long range dependencies in text, while larger n-grams leads to data sparsity issues where many n-grams are never seen in training data. The advent of neural networks allowed for more sophisticated models such as Feedforward Neural Language models \cite{bengioNeuralProbabilisticLanguage2003}, Recurrent Neural Networks (RNNs) \cite{tomasmikolavSTATISTICALLANGUAGEMODELS} and Long Short-Term Memory networks (LSTMs) \cite{merityRegularizingOptimizingLSTM2017} which could in theory capture longer range dependencies in text. However these models suffered from issues such as vanishing gradients (early tokens in a long sequence have little effect on update gradients) and information bottlenecks (the hidden state has to compress all prior context into a fixed size vector). Crucially these models failed in their flexibility to model the complex dependencies in human language.

\subsection{Modern attention based Large Language Models}

The key feature in language is context. That is every word is relatively meaningless without the context of the surrounding words. The challenge is that the way in which words relate to each other is very complex, therefore any language model needs to be able effectively capture these complex relationships. Attention is the concept where each token's representation is influenced by all the other tokens in the sequence (or just the preceding tokens in the case of autoregressive models). This was first applied to RNN-based LM in \cite{bahdanauNeuralMachineTranslation2016}, however in \cite{vaswaniAttentionAllYou2017} it was shown that attention mechanisms alone were sufficient to model language without the need for any recurrent structure.

Transformers \cite{vaswaniAttentionAllYou2017} are a class of neural network architecture that utilizes the attention mechanism to process data and generate a concise representation, or generate data autoregressively (decoder-only). The canonical transformer architecture consists of two parts an encoder and a decoder, however decoder only architectures are shown to be sufficient for all current language modelling tasks. A simplified architecture of a decoder-only transformer model is shown in Figure \ref{fig:transformer_architecture}. The key feature of the transformer is the self attention. Each token has an embedding vector which is iteratively updated by each layer of self-attention. It is done in a way that each token will only 'see' preceding tokens in the sequence. Furthermore to allow for different types of dependencies to be captured this attention is done multi times in parallel (multi-head attention) and merged together at the end of each layer. After several layers of self-attention the final set of token embeddings are used to generate a probability distribution over the vocabulary for the next token. A token can be sampled from this distribution, appended to the input sequence and the process repeated to generate long sequences of text.

\begin{figure}
    \centering
    \begin{tikzpicture}[
        block/.style={rectangle, draw, minimum width=3.5cm, minimum height=0.8cm, align=center, fill=white},
        arrow/.style={-{Latex[width=2mm,length=2mm]}, thick}
    ]

    % Input tokens
    \node[block] (input) {Input tokens $x_1, \dots, x_{t-1}$};

    % Embedding + Positional Encoding
    \node[block, below=0.8cm of input] (embed) {Embedding + \\ Positional Encoding};

    % --- Decoder Layer Internals ---
    % 1. Masked Self-Attention
    \node[block, below=2.2cm of embed] (attn) {Masked \\ Multi-Head Attention};

    % 2. Add & Norm (after Attention)
    \node[block, below=0.6cm of attn] (norm1) {Add \& Norm};

    % 3. Feed Forward
    \node[block, below=0.6cm of norm1] (ffn) {Feed Forward};

    % 4. Add & Norm (after FFN)
    \node[block, below=0.6cm of ffn] (norm2) {Add \& Norm};

    % Container for the repeating N layers
    \draw[dashed, thick] ([xshift=-0.6cm, yshift=0.6cm]attn.north west) rectangle ([xshift=0.6cm, yshift=-0.3cm]norm2.south east);
    \node[anchor=west] at ([xshift=0.7cm]norm1.east) {\Large $\times N$};

    % Output generation
    \node[block, below=1.5cm of norm2] (output) {Linear + Softmax \\ $\rightarrow$ Output token $x_t$};

    % --- Arrows and Connections ---
    \draw[arrow] (input) -- (embed);

    % Connection from Embed to Attention with QKV split and Residual
    \path (embed.south) -- (attn.north) coordinate[pos=0.35] (resid_split);
    \draw[thick] (embed.south) -- (resid_split);
    
    % Residual 1 path (branches off before QKV split)
    \draw[thick, ->, rounded corners] (resid_split) -- ++(-3.0,0) |- (norm1.west);
    \fill (resid_split) circle (2pt);

    % QKV Split
    \coordinate (qkv_split) at ([yshift=0.4cm]attn.north);
    \draw[thick] (resid_split) -- (qkv_split) node[midway, right, align=left, font=\scriptsize] {Each token's embedding is projected to \\ $Q$, $K$, $V$ with learned matrices $W_Q$, $W_K$, $W_V$};
    
    \draw[arrow] (qkv_split) -| ([xshift=-1.0cm]attn.north) node[pos=0.7, left, font=\scriptsize] {$V$};
    \draw[arrow] (qkv_split) -- (attn.north) node[pos=0.7, right, font=\scriptsize] {$K$};
    \draw[arrow] (qkv_split) -| ([xshift=1.0cm]attn.north) node[pos=0.7, right, font=\scriptsize] {$Q$};

    % Internal Layer Arrows
    \draw[arrow] (attn) -- (norm1);
    \draw[arrow] (norm1) -- (ffn);
    \draw[arrow] (ffn) -- (norm2);
    \draw[arrow] (norm2) -- (output);

    % Residual 2: Around Feed Forward
    \path (norm1.south) -- (ffn.north) coordinate[midway] (split2);
    \draw[thick, ->, rounded corners] (split2) -- ++(-3.0,0) |- (norm2.west);
    \fill (split2) circle (2pt);

    % --- Generation Loop ---
    \node[block, right=3cm of output] (gen) {Sample next token \\ and append to input};
    \draw[arrow, dashed] (output.east) -- (gen.west);
    \draw[arrow, dashed, rounded corners] (gen.north) |- (input.east);

    \end{tikzpicture}
    \caption{Simplified architecture of a decoder-only transformer model for language modeling. Adapted from \cite{vaswaniAttentionAllYou2017}.}
    \label{fig:transformer_architecture}
\end{figure}

Modern transformer based large language models (LLMs) such as GPT-3 \cite{brownLanguageModelsAre2020} are some of the largest machine learning models ever created with hundreds of billions of parameters and now trillions of parameters. Therefore training these models requires massive datasets. They are trained in a self supervised manner on large corpora (hundreds of terabytes) of text data from the internet such as Common Crawl \cite{CommonCrawl2025}, WebText \cite{Gokaslan2019OpenWeb}, and others. The objective function is usually the cross-entropy loss between the predicted token distribution and the actual next token in the sequence.

\subsection{LLM in the modern world}

Large language models (LLMs) are fundamentally just language models which can predict the next token in a sequence of text. However due to the prevalence of natural language in our world and the capabilities of LLMs they have applications in a wide variety of domains. This includes Virtual Assistants, Content creation, Code generation, Translation and many more. Furthermore the transformer architecture has been demonstrated to be effective in other domains like computer vision \cite{luViLBERTPretrainingTaskAgnostic2019} which has lead to multi-modal models that can process and generate text, images, and other data types \cite{alayracFlamingoVisualLanguage2022}. This flexibility of LLMs have lead to them have widespread adoption and proliferation in modern information systems.
Along with this proliferation there has been increasing concern about the safety and ethical implications of these models. Specifically these models have been shown to exhibit concerning behaviors like self-preservation and deception \cite{meinkeFrontierModelsAre2025c,greenblattAlignmentFakingLarge2024a,chenReasoningModelsDont2025a}. On top of this there are growing ethical questions around its use in our society \cite{benderDangersStochasticParrots2021, bengioManagingExtremeAI2024} as well as concerns of the welfare of the models themselves \cite{butlinConsciousnessArtificialIntelligence2023}.

\section{Alignment of Large Language Models}

\subsection{What is alignment?}

As discussed above large language models (LLMs) have shown remarkable capabilities in understanding and generating human-like text. However the pre-training process focuses primarily on predicting the next token in a sequence, this does not necessarily mean that the model will produce useful, safe outputs that align with what users want. Therefore, aligning LLMs is generally \footnote{Other methods are proposed which embed the alignment within the pre-training step itself. Commonly used methods include filtering/curating training data.} the process of taking the pre-trained model and fine-tuning it to better match what the designers and end users expect from the model. The aligning process can work on many dimensions of the models behavior from helpfulness, politeness, honesty, and safety to more complex values such as fairness, inclusively, and ethical considerations. More powerful models have a greater potential to cause harm if not properly aligned, making alignment along the dimensions such as safety and ethical considerations increasingly important. In this section I will outline some of the most common methods for post-training alignment of large language models along with current limitations and other methods.

\subsection{Alignment through supervised fine-tuning}

A common first step of alignment is supervised learning where a dataset of prompts and expected response is used to fine-tune the pre-trained model. For most of preference optimization methods outline below supervised fine-tuning is used as the first step to create a base model \cite{ouyangTrainingLanguageModels2022,rafailovDirectPreferenceOptimization2024,baiConstitutionalAIHarmlessness2022,leeRLAIFVsRLHF2024}. Depending on the dataset used the SFT step can help the model become better at following instructions \cite{ouyangTrainingLanguageModels2022}, domain specific knowledge and style \cite{liChatDoctorMedicalChat2023}, induce Chain-of-Thought reasoning \cite{weiChainofThoughtPromptingElicits2023}, change the values of the model \cite{nieSurveytoBehaviorDownstreamAlignment2025}, and many other capabilities. The key idea is that because the LLM already has a strong grasp of language and knowledge from pre-training, the SFT step can be used to nudge the model towards the desired behavior at a higher abstraction level than the predominantly low level token prediction task used in pre-training.

\subsection{Alignment through preference optimization}
A common element in most post-training alignment methods is the use of preference optimization. The idea is to use a dataset of prompts and potential responses that are ranked either by humans or AI models to indicate which responses are preferred over others. 

This preference data is usually (but not always) in the form of pairwise comparisions. That is given a prompt $x$, a pair of responses $i$ and $j$ the labellers (AI or human) indicate which response is perferred for example $i \succ j$ indicates response $i$ is preferred over response $j$. If we have a dataset of these pairwise comparisons we can use the Bradley-Terry model \cite{bradleyRANKANALYSISINCOMPLETE1952} to create a maximum likelihood loss fuctuion
\begin{align}
    P(i \succ j \mid x) &= \frac{\pi_i}{\pi_i + \pi_j}, \\
    \mathcal{L}_{\mathrm{BT}} &= - \sum_{(x, i, j) \sim D} \log P(i \succ j \mid x) \\
    &= - \sum_{(x, i, j) \sim D} \log \frac{\pi_i}{\pi_i + \pi_j} \label{eq:bradley-terry-loss}
\end{align}
Where $\pi_i$ and $\pi_j$ are the scores of responses $i$ and $j$ respectively given prompt $x$. The way these scores are calculated depends on the specific preference optimization method used.

The success of preference optimization methods relies on the ability of modern LLMs to generalize learned preferences to new contexts and prompts not seen during training. The distribution of prompts in the preference dataset is crucial as it defines the context for which the model is expected to generalize its learned preferences. Therefore it important to ensure that the prompts used to create the preference dataset are sufficiently representative of the contexts and dimensions in which you are expecting the model to align its behavior. Furthermore the rankings of the responses is also important as they define what the model is aligning towards. In many cases these rankings are subjective and provided by a small group of human labelers (n~50).

\subsection{Reinforcement Learning from Human Feedback}
Reinforcement Learning from Human Feedback (RLHF) \cite{ouyangTrainingLanguageModels2022} uses human labellers to create the preference dataset and then a reinforcement learning algorithm to optimize the model based on this feedback. This method popularized post-training alignment and was used to train InstructGPT which was smaller and better at following instructions compared to GPT-3 \cite{brownLanguageModelsAre2020, ouyangTrainingLanguageModels2022}.

\subsubsection{How does RLHF work?}

RLHF is separated into three distinct stages. Firstly is supervised fine-tuning (SFT) where a pre-trained language model is fine-tuned on a dataset of prompts and human written 'expected' responses to those prompts.

The second step is collecting multiple responses from the SFT model to create a set of prompts and response then having human labelers rank these responses from best to worst. This ranked data is then used to train a preference model (PM) that can predict which of two responses is better aligned to human preferences. The preference model is trained using a Bradley -Terry loss function where the scores $\pi_i$ is equal to $\exp(\text{PM}_\theta(x, i))$. This is fed into \ref{eq:bradley-terry-loss} to create the loss function for the preference model.

Finally the fitted PM is used as a reward signal in the PPO \cite{schulmanProximalPolicyOptimization2017} RL algorithm to further fine-tune the SFT model and create the final RLHF model.

\subsubsection{Relevance}

The idea of using human feedback to help train machine learning models is powerful, although it is not new and was around in theory and concept for years beforehand \cite{christianoDeepReinforcementLearning2017}. The specific method of RLHF was widely adopted \cite{team2023gemini,baiQwenTechnicalReport2023,shaoDeepSeekMathPushingLimits2024} yet is now replaced with more effective methods to utilize human feedback such as Direct Preference Optimization (DPO) \cite{rafailovDirectPreferenceOptimization2024}.

\subsection{Direct Preference Optimization}

The idea presented in \cite{ouyangTrainingLanguageModels2022} introduces the concept of using human feedback to train large language models to be aligned to human preferences. However the process of RLHF is complex and requires training multiple models (SFT, PM, RLHF). Direct preference optimization (DPO) \cite{rafailovDirectPreferenceOptimization2024} is a method that simplifies this process by removing the need for reinforcement learning altogether. Instead DPO reformulates the preference modelling step to instead create an objective function that implicitly creates the reward function from the model and a reference model (usually the SFT model).

\subsubsection{How does DPO work?}

DPO starts with the same preferences data as RLHF; a set of prompts with responses ranked by humans. Like RLHF it starts with a pre-trained language model and is supervised fine-tuned on the prompts and highest ranked responses to create an SFT model. Instead of training a seperate preference model DPO uses the SFT model as a reference model and defines the reward function $\hat{r}_\theta(x,y) = \log \pi_\theta(y|x) - \log \pi_{SFT}(y|x)$. Then be some derivation \cite{rafailovDirectPreferenceOptimization2024} it can be shown that the following loss function gradient can be used to optimize the Bradley Terry model directly on the preference data:
\begin{equation*}
\begin{aligned}
\nabla_\theta \mathcal{L}_{\mathrm{DPO}}(\theta)
&= -\mathbb{E}_{(x, y^+, y^-) \sim D} \big[ \\
&\quad
\underbrace{
\sigma \!\left(
\hat{r}_\theta(x,y^-)
-
\hat{r}_\theta(x,y^+)
\right)
}_{\text{Reward model correctness}}
\left(
\underbrace{
\nabla_\theta \log \pi_\theta(y^+|x)
}_{\text{Increase preferred likelihood}}
-
\underbrace{
\nabla_\theta \log \pi_\theta(y^-|x)
}_{\text{Decrease dispreferred likelihood}}
\right)\\
&\big]
\end{aligned}
\end{equation*}

\noindent Where $\sigma$ is the sigmoid function, and $(x, y^+, y^-)$ are the prompt, preferred response, and dis-preferred response respectively. \cite{rafailovDirectPreferenceOptimization2024} found that the reward model correctness weighting term for the update is very important. This update can be calculated with only 4 forward passes through the network per preference pair (2 for reference model and two for current model), making it much more efficient than RLHF \footnote{RLHF will need to do many forward passes of the network to train the preference model, then it will still have to do 3 forward passes for each datapoint when it comes to the RL step.}.

\subsubsection{Relevance}

DPO provides a more effective way to utilize the same underlying feedback data that is used in RLHF. It replaced RLHF as the baseline method for post-training alignment. Due to its widespread adoption DPO represents a strong baseline for alignment.

\subsection{Kahneman-Tversky Optimization}

Traditional preference optimization methods such as RLHF and DPO set out with the objective of making the model maximize the likelihood of preferred responses over dispreferred responses. KTO \cite{ethayarajhKTOModelAlignment2024} sets out to change the objective of a language model to maximize the perceived utility of generations, which is how useful the generation is for the end user. The key being perceived utility is not the same as actual utility due to cognitive biases discussed in the follow sections. Ethayarajh et Al introduces the concept of Human-aware losses (HALOs) which optimize for what human perceive as the best outcome. It is shown that DPO and PPO are both HALOs which can help explain their success. Furthermore Ethayarajh et Al proposes a new objective function from the HALO family that works on only binary preference data\footnote{Given a prompt and response a labellers just need to label the response as desirable or undesirable}, making data collection easier and more natural.

\subsubsection{Prospect Theory}

KTO is built off the foundation of prospect theory \cite{tverskyAdvancesProspectTheory1992}  which is a theory to explain how humans make decision when dealing with uncertainty and risk \footnote{For example why when humans are given the choice between \$50 and a 50\% chance to win \$100, they often prefer the sure \$50, when they are both worth the same amount. Furthermore this effect is still present when the gamble is worth even more (\$110) \cite{kahnemanProspectTheory1979}}. It explains several cognitive biases that humans have when making decision involving risk, namely loss aversion (losses hit harder than gains), probability weighting (overweight small probabilities and underweight large probabilities), and reference dependence (we evaluate outcomes relative to a reference point which is usually the status quo). In relation to a LLM this means that when a human is evaluating the response of the model they are going to perceive the quality (utility) of the response in a biased way, so using a objective function that takes into account (Human-aware) these biases should lead to better alignment.

\begin{figure}[h!]
\centering
\begin{tikzpicture}
    \begin{axis}[
        axis lines=middle,
        xlabel={True Value $x$},
        ylabel={Perceived Value $v(x)$},
        xmin=-1.2, xmax=1.2,
        ymin=-2.2, ymax=1.2,
        xtick=\empty,
        ytick=\empty,
        width=12cm,
        height=8cm,
        legend pos=north west,
        domain=-1:1,
        samples=200
    ]
    
    % Kahneman-Tversky curve (losses convex, gains concave)
    \addplot[red, thick, dashed] {x>=0 ? x^0.45 : -1.5*(-x)^0.45};
    \addlegendentry{Kahneman-Tversky}

    % Reference point
    \node at (axis cs:0,0) [anchor=north east] {Reference point};
    
    \end{axis}
\end{tikzpicture}
\caption{Implied human value functions showing loss aversion: concave for gains (risk averse) and convex for losses (risk seeking). Adapted from \cite{tverskyAdvancesProspectTheory1992}, with a change of terminology to match the context of value alignment and exaggeration for illustration.}
\label{fig:prospect_theory}
\end{figure}

Prospect theory does introduce several key concepts. Firstly is each random variable $Z$ (the LLM output) has a utility which is determined by $\sum_{z \in Z} w(z) v(z-z_0)$. Secondly is the weighting function $w(z)$ which is used to weight the probabilities of each outcome $z$ in a biased way. Finally is the value function $v(z-z_0)$ which determines how good/bad an outcome is compared to a reference point $z_0$, this also will also include some bias from the individuals perspective.

\subsubsection{How KTO works}

KTO applies the idea of prospect theory by modifying the canonical utility function (shown in \ref{fig:prospect_theory}) provided in \cite{tverskyAdvancesProspectTheory1992} to work for a language model fine tuning objective function. Namely it does his by omitting the weighting function (as users only see the final response, not the full probability distribution), making some minor adjustments for computational stabilty and using the Kullback-Leibler divergence \cite{kullbackInformationSufficiency1951} (between current model and reference model which is usually the SFT model) as a dynamic reference point. Notably this means that KTO only needs a database of prompts and responses labelled as either desirable or undesirable ($d=1$ for desirable, $d=-1$ for undesirable) where the reference point is neutral ($0$). The KTO loss function is defined as:

\begin{equation*}
\mathcal{L}_{\mathrm{KTO}}(\pi_\theta, \pi_{\text{ref}}) = \mathbb{E}_{(x, y, d) \sim D} \left[ \lambda(d) - v(x,y)\right]
\end{equation*}
\noindent where
\begin{equation*}
\begin{aligned}
v(x,y) &=\begin{cases}
\underbrace{\lambda_D \sigma(\beta(r_\theta(x,y)-z_0))}_\text{increase probability above reference point}, & \text{if } d = 1 \\
\underbrace{\lambda_U \sigma(\beta(z_0 - r_\theta(x,y)))}_\text{decrease probability below reference point}, & \text{if } d = -1 \\
\end{cases} \\
\\
\lambda(d) &=
\begin{cases}
\lambda_D, & \text{if } d = 1 \\
\lambda_U, & \text{if } d = -1 \\
\end{cases}
\\
r_\theta(x,y) &= \log \pi_\theta(y|x) - \log \pi_{\text{ref}}(y|x) \quad \text{Same reward model as in DPO}\\
z_0 &= D_\text{KL} \left( \pi_\theta(y|x) \| \pi_{\text{ref}}(y|x)\right) \quad \text{Dynamic reference point}
\end{aligned}
\end{equation*}
The three  hyperparameters $\beta$ is a risk aversion parameter, and $\lambda_D$, $\lambda_U$ are scaling parameters for desirable and undesirable responses respectively. In the references implementation and experiments $\lambda_D = \lambda_U = 1$ and the reference model is a SFT model.

\subsubsection{Relevance}

KTO offers several key logistical and theoretical advantages over prior preference optimization methods, that make it an attractive method for alignment.
Firstly the logistical advantage. Collecting the ranked preference data which is needed by normal alignment methods is normally quite expensive \cite{casperOpenProblemsFundamental2023} where binary like/dislike information is much more natural and less invasive to collect. KTO is shown to be more efficient and more robust to data imbalances than DPO. Furthermore a model of sufficient size (~13B) does not need a SFT step prior to KTO which greatly reduces computational requirements. Furthermore KTO shows similar performance without using a reference model \footnote{This is done by assuming that the reference model returns a uniform distribution across all outputs given $x$} which also halves the memory requirements when training.
Secondly are the theoretical advantages. KTO is the first alignment method to explicitly take into account human cognitive biases when optimizing for alignment.

Another key point is the concept of applying prospect theory to alignment. Ethayarajh et Al apply it in the context of preference optimization, however propect theory and the biases in human cognition can be applied more broadly than that in alignment. \todo{There is something here yet I am not quite sure. One could change the axises in \ref{fig:prospect_theory} to represent the concept of actual values and perceived values (however this feels like trying to shoehorn it in too much when teh concept of revealed preferences is more appropriate). Another idea is that prospect theroy explains how users wil provie feedback ina biased way and we should actualy be using the prospect theyr to map that to the tryue thing we shold align for, might help with g eneralisation.}

\subsection{Constitutional AI}

\subsubsection{What is Constitutional AI?}

Constitutional AI (CAI) is a method of post-training a large language model to be aligned to set of principles that are outlined in simple natural language document called a constitution \cite{baiConstitutionalAIHarmlessness2022}. Importantly it works without the need for human labelling of data common in other alignment methods such as RLHF and DPO \cite{ouyangTrainingLanguageModels2022, rafailovDirectPreferenceOptimization2024}. Instead of human labelling CAI uses the model itself to generate feedback on its outputs based on the principles outlined in the constitution, this feedback is then used to further train the model to align it to the constitution.

\subsubsection{How does Constitutional AI work?}

CAI is a process that takes in a constitution, a pre-trained language model, and a dataset of prompts and outputs a model that is aligned to the constitution. The process consists of three main steps: self generative and supervised fine-tuning (SFT), preference modelling (PM), and reinforcement learning with human feedback (RLHF) \cite{baiConstitutionalAIHarmlessness2022}. The constitution itself was generated in an adhoc manner by the authors of \cite{baiConstitutionalAIHarmlessness2022} and consists of a set of principles that aim to make the model's outputs more helpful, honest, and harmless.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        scale=0.7, transform shape,
        node distance = 2cm and 2.5cm,
        every node/.style={font=\small},
        diamond/.style={draw, shape=diamond, aspect=1.5, align=center, fill=blue!15, blur shadow},
        rect/.style={draw, rectangle, rounded corners=6pt, align=center, fill=red!15, blur shadow},
        smallrect/.style={draw, rectangle, rounded corners=6pt, align=center, fill=yellow!20, minimum width=2.2cm},
        process/.style={draw, rectangle, rounded corners=6pt, align=center, fill=green!20, blur shadow},
        stage/.style={draw, rectangle, rounded corners=6pt, align=center, fill=purple!20, blur shadow},
        arrow/.style={-Latex, thick}
    ]

    % Top left diamond
    \node[diamond] (input) {Input pretrained\\Model};

    % Harmful sample generation
    \node[rect, right=of input] (redteam1) {Generate Responses\\to ``Red Teaming''\\Prompts Eliciting\\Harmful Samples};

    % response critique revision boxes
    \node[smallrect, above right=0cm and 1.5cm of redteam1] (resp) {Response};
    \node[smallrect, below=0.3cm of resp] (crit) {Critique};
    \node[smallrect, below=0.3cm of crit] (rev) {Revision};

    % SL-CAI diamond
    \node[diamond, right=4.8cm of redteam1] (slcai) {Finetuned\\SL-CAI\\Model};

    % Lower left harmful pairs
    \node[rect, below=2cm of input] (redteam2) {Generate Responses\\to ``Red Teaming''\\Prompts Eliciting\\Pairs of Samples};

    % constitutional AI feedback
    \node[stage, right=1cm of redteam2] (cai) {Constitutional AI Feedback\\for Self-Improvement};

    % preference model
    \node[process, right=1cm of cai] (pm) {Finetuned\\Preference Model\\(PM)};

    % RL-AIF model block
    \node[process, right=1cm of pm] (rlaif) {RLAIF Training with\\PM + SL-CAI\\Models};

    % final RL-CAI
    \node[diamond, right=1cm of rlaif] (final) {Final\\RL-CAI\\Model};

    % arrows
    \draw[arrow] (input) -- (redteam1);

    \draw[arrow] (redteam2) -- (cai);
    \draw[arrow] (cai) -- (pm);
    \draw[arrow] (pm) -- (rlaif);

    \draw[arrow] (slcai) -- (rlaif);
    \draw[arrow] (slcai) -- (redteam2);
    \draw[arrow] (rlaif) -- (final);

    % arrows from critique-response-revision group
    \draw[arrow] (redteam1.east) -- ++(0.8,0) |- (resp.west);
    \draw[arrow] (resp) -- (crit);
    \draw[arrow] (crit) -- (rev);
    \draw[arrow] (rev.east) -- ++(0.7,0) |- (slcai.north);

    \end{tikzpicture}

    \label{fig:cai_process}
    \caption{Overview of Constitutional AI process, adapted from \cite{baiConstitutionalAIHarmlessness2022}}
\end{figure}

\subsubsection{Supervised learning on revised responses}

The first stage of the process involves taking the pre-trained model and using it to generate responses to a set of prompts. These prompts are designed to elicit harmful behavior. As the pre-trained model is designed to be helpful it will likely generate harmful responses to these prompts. These responses are then critiqued by the model itself using principles outlined in the constitution, only a subset of which are used for each response critique. The critique is then used to revise the original response to make it more aligned to the constitution. This process of generating responses, critiquing them, and revising them is done iteratively to create a dataset of revised responses. This dataset is then used to fine-tune the pre-trained model using self-supervised learning to create a model called the SL-CAI model.

\subsubsection{Reinforcement learning with AI feedback}
The next step involves using the SL-CAI model to generate pairs of responses to a set of prompts (overlapping with the set of prompts from before or not). Then using the constitution the SL-CAI model is used to provide feedback on which of the two responses is better aligned to the constitution. This feedback is then used to fine-tune a preference model (PM) that can predict which of two responses is better aligned to the constitution. Finally the PM and SL-CAI models are used together to do reinforcement learning in style of RFHL \cite{ouyangTrainingLanguageModels2022} to generate the final model RL-CAI. However the use of RLHF is not needed and other more modern methods like KTO and DPO can be employed This final model is now aligned to the constitution without any human labelling of data. In the setup used in \cite{baiConstitutionalAIHarmlessness2022} the authors did in fact use some labels from human labelers which were used to provide the helpfulness signal in the preference model training, however in principle the preference signal could have been entirely generated by an AI model.

\subsubsection{Relevance}

In the context of when this paper was released there was a key dilemma of helpfulness vs harmlessness in large language models. Therefore, this paper set out to make a model that is both helpful and harmless, without sacrificing one for the other. Further research has expanded the horizons to be concerned with more than just harmfulness vs helpfulness trade off. As hinted at by the authors in \cite{baiConstitutionalAIHarmlessness2022}, there is room for improvement in the CAI method to make it focus on more than just harmlessness vs helpfulness. This can be done by modifying the constitutions to include more diverse principles as well as modifying the red teaming prompts to target more diverse scenarios (i.e controversial topics, political leanings, etc). Collective Constitutional AI \cite{huangCollectiveConstitutionalAI2024} is the process of generating the constitution through public consultation.

\subsection{Evaluation of Large Language Model alignment}

\todo[inline]{Talk about how we really care about how the flow on effects are. I.e societal impacts, individual impacts of the model. Yet what we are measuring are single model ouputs (in general). Talk about how we can measure these flow on effects better. Talk about the limitations of current evaluation methods.}

\subsection{Current technical alignment limitations}

The goal of alignment is to ensure that you create a model that is aligned with what the designer and end users want. However there are several challenges that make this difficult namely being alignment data and effectiveness of alignment methods. Furthermore there is also a prevalent background concern of what/who the model is being aligned towards.

Preference optimization is the most used method of alignment which requires a dataset of prompts and preferred responses (ranked, or binary). There are several challenges with this data. Namely the cost of collecting it can be prohibitive \cite{leeRLAIFVsRLHF2024,casperOpenProblemsFundamental2023} and secondly is the concern of of what/who the model is being aligned towards.

Current alignment methods demonstrate success on certain metrics that there methods work, that is they can make model more helpful, more honest, and less harmful \cite{ouyangTrainingLanguageModels2022,baiConstitutionalAIHarmlessness2022,rafailovDirectPreferenceOptimization2024,ethayarajhKTOModelAlignment2024}. Yet the metrics used (broadly either human or AI evaluation of paired responses) miss out on deeper concerns around alignment. The first concern is that current methods may only achieve \textit{shallow alignment} that is the model appears aligned during training yet fails to generalize this alignment to new contexts. Another concerns is \textit{inner alignment} where the training process is aligned (outer-alignment) yet the model itself has learned objective that are misaligned with the intended objective. Finally we observe models to make sure they perform how we expect them too, therefore we need \textit{scalable oversight} methods to ensure that as models become more capable we can still ensure they are behaving correctly. These concerns are not new concerns in AI safety \cite{amodeiConcreteProblemsAI2016a} yet they remain open problems in the context of large language models and their alignment.

\section{Desiderata for AI}

Alignment is a key concern in AI safety as we want to ensure that AI systems act and behave in a way that aligns with what humans want. More broadly we are at a transitional point in the history of Artificial Intelligence where the technology, systems and institution we build around AI have the potential to shape the future of humanity. Therefore it is paramount that we understand the context around AI, ways in which AI could bring about net negative change and the way in which we would like AI to built and integrated into society.

\subsection{Goal of AI}

Anytime we add something to the world we should ensure that it is worth adding and going to provide at least net positive benefit to humanity. Measuring net positive benefit is hard and requires a metric that we can measure the positive and negative impacts of AI on humanity with. A complete discussion and outline of such a metric is out of scope for this section, furthermore a single perfect "goal" is not likely to exist \cite{amodeiConcreteProblemsAI2016a} and if it did wouldn't be a useful goal due to Goodhart's law \cite{Goodhart1975}. However we can outline some high level ideas of what a good goal for the concept of AI could be, with the caveat and understanding that this is not a perfect goal and simply some high level guidance.

A good starting point for a "goal of AI" is the idea of human flourishing \cite{vanderweelePromotionHumanFlourishing2017} that is split into 5 domains that are near universally agreed upon as good things for humans to have; happiness and life satisfaction, mental and physical health, meaning and purpose, character and virtue, close social relationships. 
\begin{definition}
The goal of AI should be to help maximize the median human flourishing across all people currently alive and in the future. \footnote{Including non humans at the root level of the goal is plausible. However one could reach the same conclusions of animal welfare by maximizing the human flourishing of "animal concerned" humans.}
\end{definition}

We can see the utility of this goal as it is broad enough to allow each individuals own interpretation of what human flourishing means to them, while still allowing us to understand harms and benefits of various AI systems. More discussion is found in a later section on how to understand human values.

\todo[inline]{Feel like I need to add something more here that outlines what is currently missing from the goal of AI. Like a little critique of the status quo.}

\subsection{Stakeholders of AI}

if we are on the precipice of a new technological revolution we need to understand what and importantly who is at stake. artificial intelligence has the potential to impact everyone on the planet, an american firm that utilizes ai to handle customer service can take away jobs from urban workers in india. it can also have a impact through time, an ai paperclip factory that turns everyone into paperclips would extinguish humanity in the future and remove all possible positive future for humanity. ignoring certain groups of humans when designing and building ai systems leads to optimization for the benefit of one group over the other. pressingly this happens when we ignore future people and take short term gains and push the costs and risks onto future generations or when we extract value from marginalized groups to benefit a select few \footnote{like modern day colonialism where the worlds rich extract data, rare earth materials and labour from the worlds poor and use it for their own gain. \cite{crawfordAtlasAIPower2021}}.

There are many stakeholder of AI and to ensure that they are all represented we need to have the concepts of non-exclusion and non-domination. This means that no group of people should be excluded from the benefits of AI and no group of people use of AI should dominate over another group of people use of AI. When the groups are globally and temporally separated strict adherence to these principles could ensure that AI is atleast evenly distributed across all people.

The problem however is when not all the stakeholders of AI are considered or empowered. Looking at the example of the customer service AI which will be designed, developed and deployed by a company for use by other companies. In a fair market it will make both companies rich at the expense of the workers who have lost their jobs. AI systems are overwhelming targeting companies as their 'users' which leads to companies driving the development and deployment of AI systems with disregard of the negative externalities of the true stakeholders.

\subsection{Alignment of AI}

The actions of a few can impact the many. AI is a technology that can multiple the impact of ones actions. Therefore we need to ensure that everything around AI is aligned to the best interests of all stakeholders of AI. Yet due to no universal truth to govern what is best for all stakeholders we need to have AI that is aligned to the collective best interests of all stakeholders of AI. It is unclear what the method of collective alignment would be. Different subgroups of the stakeholders have different values and goals, aligning collectively therefore means finding a way to have a target which can be done in multiple ways.
\begin{enumerate}
    \item \textbf{Pluralism: } Different groups will have different values. We have no ground truth to guide us on which values are better than the other. Therefore we should align models to be pluralistic in nature and make decisions that reflect this diversity along with the uncertainty about which values are most important. This can result in multiple "aligned" models or one model that can reflect different values in different contexts. Most aligned with the idea of agnostic democracy \cite{Wenman_2013}.
    \item \textbf{Aggregation: } With the different sets of values from different groups we can aggregate these values to find a middle ground (naive averaging) however this can lead to poor compromises and tyranny of the majority \cite{millLiberty2011,tocquevilleDemocracyAmerica1835}. Therefmore more complex aggregation that weight different groups values by impact, vulnerability and importance can be used to find a better middle ground.
    \item \textbf{Conglomeration: } If one assumes that all people can mostly agree than we can use idea from deliberative democracy \cite{habermasFactsNormsContributions1998} to find a set of values that all people can agree on \footnote{All people agree can be operationally defined as "maximum equal approval" \cite{jonathanstrayPracticalDefinitionPolitical}}. This is where methods like citizen assemblies and tools like polis \cite{compdemocracyPolis2025} can be used to find a set of values that all people can agree on.
\end{enumerate}

Regardless of the method of aligning to the stakeholders, the current goal of AI is misaligned with this definition as the goal of most AI systems is to maximize profit for companies and shareholders. It shouldn't take excessive development to understand the negative externalities of profit motives \cite{stiglitzMarketsMarketFailures1989}. The problem is not that GPT-4 itself is aligned to the goal of profit maximization, but that is second order effects (company share holder profit) that cause the development and deployment of the AI system to maximize profit. A related demonstration of this misalignment is the proliferations of recommender systems. These machine learning models that started with the intention of helping users find content they enjoy, relevant information have turned into systems that prioritize engagement (and therefore ad revenue) over user flourishing \cite{milanoRecommenderSystemsTheir2020}. We can see that the incentives of the institutions and frameworks that built the systems \textit{captured} the original incentive of the AI system which could of been "help the user", "show the user something they will enjoy". This is why it is important not just to align the AI system itself, but also the institutions and systems around AI to be aligned to human values and goals. Full stack alignment \cite{edelmanFullStackAlignmentCoAligning2025} is this idea exactly where you need to make sure that individual models, oversight models and everything in the middle are all aligned to the target that we want.


\todo[inline]{Look into some philosophical concepts around how the collective the best baseline if we have no other gold standard. Provide some rationale as to why using the public as the source of values is a good idea, as opposed to some "gold standard" values provided by experts.
This will lend in the public AI. As AI is for the public, for the greater good etc.
}

\subsection{How to do AI}

If AI has any chance of achieving the goal of maximum average human flourishing we need to ensure that it is done in a way that is aligned to this goal. Full stack alignment is the idea of this however each part of the stack needs to be defined. A key idea here however is that due to objective capture the higher up the stack the alignment is the more influential the alignment is. Therefore aligning the institutions and frameworks which govern and build AI systems has the highest leverage for ensuring that AI is built in a way that is aligned to human values and goals.

Leaving AI development to private companies driven by profit might be effective in some ways, drive rapid development yet it is fundamentally misaligned with the goal of maximizing human flourishing (especially in the long term). A better framework and systems around AI development is needed. Currently a democratic framework rules the world as the most progressive and forward thinking way to govern large groups of people.
\begin{quote}
``Democracy is the worst form of government, except for all the other forms that have been tried from time to time.''\cite{churchillwinstonsSpeechHouseCommons1947}.
\end{quote}
Even though democracy has it flaws it is arguably the most admissible way to govern. Therefore using democratic ideals and frameworks to also govern, spearhead and deploy AI is a good starting point for ensuring that the goal of AI is upheld.Operationally this means making artificial intelligence a public good that is governed and operated by the public for the public. \cite{publicaiPublicAIWhite2024}

Public AI provides a good that is both non-dominate and non-exclusionary. This method of making it a public good provides has three key features; public access, public accountability and permanent availability. Public access ensure that the opportunities to use AI and receive its benefits are available to all people. Public accountability crucially ensure that the stakeholders of AI (all people) have a say in how AI is developed and deployed, which allows AI the opportunity to attain the goal of maximizing median human flourishing. Finally the permanence feature allows it to be practically used and built up in much the same way that the electrical grid or internet is built and maintained.

There are two important failure modes however in this public approach. One is economic failure where the public good is not economically sustainable and therefore turns to undesirable methods of funding (ads, data extraction etc) \footnote{Which is very much similar to the way that the internet was developed. It started off funded by the public sector (laying cables etc), however then for anyone to have a successful business offered on the internet it had to be "free" to use and so companies (E.g Google, Facebook) needed to find alternative methods of funding which has proven detrimental to greater public health}.The second is more of a global coordination/cooperation failure, even if all AI in the world is public AI one country could still prioritize its own interests over the collective good.
\section{Problems of AI}

AI alignment is a key concern in AI safety. However there are several other concerns that need to be addressed to ensure that AI is safe and beneficial for humanity.

\subsection{Concrete problems of AI safety}

negative side effects sometimes we wantt hte model to do things yet sometimes we don't want it to do thinkg

Mis generalization how we might align and then it operates different in tet

Scalable oversight as models getm ore powerful how to know if they are being alright

Robustness to distributional shift when the model is in a new environment how to ensure it is still aligned

\subsection{Catastrophic risks of AI}
https://arxiv.org/abs/2306.12001
https://futureoflife.org/resource/catastrophic-ai-scenarios/

Even if style framing. There are still some great risks of AI.

Runaway self-improvement

Weapons of mass destruction (cyber, chemical etc)

Mis aligned AI

\subsection{Preventing catastrophic risks of AI}

Stopping should we do 
Stopping can we do it

Deep alignment (the topics here)

\todo[inline]{Talk about general overarhcing problems of AI safety. \cite{amodeiConcreteProblemsAI2016a} is a good starting point. Talka bout negative side effect, mis generalization, scalable oversight and robustness. This is also a good page to look from: https://www.princetonalignment.org/papers. Also other problemsl ike value lock in etc}


\section{Representing Human Values for LLM alignment}

As discussed in the previous sections it is paramount that we make sure that large language models (LLMs) are aligned to human values and by extension behavior. There remains a key questions of how do we represent and understand human values that would allow us to align a AI system to them. There are broadly two methods one works from defining broad principles and specific actions are inferred from there the other method is providing specific examples and generalizing from them. Interestingly when working with humans we can only really tell them in a high level what we want their values to be and let them infer the specifics, yet with modern AI systems like LLMs we can provide lots of specific examples and let it infer the values from this \footnote{Providing a human with 10,000 different questions and responses of "how to answer the phone" won't provide much clarity, yet giving them some simple high level rules is quite effective. Oppositely for AI most alignment methods like RLHF, DPO etc provide lots of specific examples and let the model infer the underlying values.}. In this section I will outline some of the key concepts around representing human values and how they relate to AI systems.

\subsection{Human values taxonomy}

Human values are an old idea that go back millennia \cite{aristotleAristotlesNicomacheanEthics350B.C.E,mullerLawsManu1886,amesAnalectsConfuciusPhilosophical1998}, with each culture having its own set of values that are important to them. They define how people should be and act \footnote{Although the actual definition of values is up for contention, this simple version can suffice for our purposes}. In modern times we have developed various taxonomies of humans values tha allow us to understand and categorize different sets of values.

A widely cited modern value taxonomy is Rokeach's book "The Nature of Human Values" \cite{rokeachNatureHumanValues1973} where he posits that humans values can be understood as 36 distinct values. Rokeach groups the values are either \textit{terminal values} which are desirable end-states (freedom, equality...) and \textit{instrumental values} which are preferable modes of behavior (honesty, kindness...). Expanding on this idea of value taxonomy Schwartz \cite{schwartzUniversalsContentStructure1992} proposes a model of 10 broad human values that he later expands to 19 values \cite{schwartzRefiningTheoryBasic2012}. Schwartz's organises the values in a circumplex structure where one dimensions is self-enhancement vs self-transcendence and the other is openness to change vs conservation.

These taxonomies provide a useful framework for understanding human values and how they relate to each other. Importantly they are trying to provide the most predictive ability of peoples actions given values with the least number of values. This trade off between complexity and predictive power is important when trying to represent human values in AI systems. Contemporary work from computer scientists have created value taxonomies that are designed specifically for their predictive \footnote{Which in this case is the ability for a model to predict someones values (from the taxonomy) given an argument that they made. Plausibly the word modelling power might be more intuitive.} power \cite{kieselIdentifyingHumanValues2022}. Yet we can see that it is more complex (54values) and not as interpretable as the psychological taxonomies. This means that it works well for AI systems yet is hard to apply to humans. This means that for the purpose of AI alignment we need a taxonomy that is both interpretable and has good predictive power that would allow us to align AI systems to human values effectively.

\subsection{Value surveys}

One of the key features of early modern value taxonomies is that they were created with associated survey instruments that allowed researchers to measure the values of individuals. Rokeach \cite{rokeachNatureHumanValues1973} created the Rokeach Value Survey (RVS) which asked participants to rank the 36 values in order of importance to them. Schwartz \cite{schwartzUniversalsContentStructure1992} created the Schwartz Value Survey (SVS) which asked participants to rate the importance of each value on a 9-point scale. Later Schwartz replaced the SVS with the Portrait Values Questionnaire (PVQ) \cite{schwartzOverviewSchwartzTheory2012} which was less abstract and better at soliciting the values from a diverse set of people as respondents are answering questions of the form "How much is this person like you", and by rating their similarity with 40 different portraits one can infer someones values. These surveys allowed researchers to measure the values of individuals and groups and provided a way to understand how values relate to behavior.

All the above surveys (PVQ, SVS and RVS) are designed to measure the values of individuals in relation to be a predefined taxonomy of values. They have relatively few questions (<50), abstract in nature and quick to complete. An alternative path for measuring values is to create broader surveys that try to capture the values of large groups of people through many questions. Importantly these surveys do not map directly to a predefined taxonomy of values yet instead try to capture the complexity of human values through many questions (~300). The World Values Survey \cite{WVS2020} is the biggest example of this where they have been surveying over 1,000 people from over 100 countries every 5 years since 1981. The New Zealand Attitudes and Values Study (NZAVS) \cite{NZAVS2025} is another example of a large scale value survey that has been running since 2009 surveying around 30,000 New Zealanders every year. These surveys don't correspond to a strict values taxonomy yet instead provide granular data from each individual. This data can help train \cite{nieSurveytoBehaviorDownstreamAlignment2025,liCultureLLMIncorporatingCultural2024} and evaluate \cite{durmusMeasuringRepresentationSubjective2024,zhaoWorldValuesBenchLargeScaleBenchmark2024,aroraProbingPreTrainedLanguage2025}Large Language Models (LLMs) on human values.

These surveys provide granular data yet struggle from well documented issues of self-reporting bias \cite{podsakoffCommonMethodBiases2003} and social desirability bias \cite{crowneNewScaleSocial1960}. This means that the values reported by respondents may not accurately reflect their true values. From an economics perspective this is the distinction between stated values and revealed values. Stated values are those that people report in surveys, while revealed values are those that can be inferred from their behavior. This provides a caveat and limitation to using value surveys as a way to represent human values for AI alignment.

\subsection{Values to behavior}
For the purposes of building AI systems understanding values is only useful as far as they can predict behavior (in the form of responses and actions). That is we want the AI system to behave the same or better in all situations that a human would. There is contention in psychology and AI alignment research around the values to behavior gap \cite{kollmussMindGapWhy2002,gabrielArtificialIntelligenceValues2020}. When one tries to bridge the gap between understanding someones behavior and values we run into common problems namely; non-identifiability of values (Many different values can lead to the same behavior), specification gaming (as mentioned above) and extrapolation errors. Non-identifiability and extrapolation errors are demonstrated clearly in inverse reinforcement learning (IRL) \cite{abbeelApprenticeshipLearningInverse2004}. Furthermore it can be shown that human values are fickle and can stem from ones previous actions as humans rationalize what they have done \cite{leonfestingerTheoryCognitiveDissonance1957}. This is the reasons as to why many AI alignment researchers focus on direct behavior alignment through revealed values with pairwise preference ranking methods as discussed above like DPO, RLHF and KTO.

\todo[inline]{Use some of the reference from \cite{klingefjordWhatAreHuman2024} and otherresearch to talk more about values and different definitions.}

However the idea that values can lead to and predict behavior has been argued \cite{russellh.fazioDirectExperienceAttitudeBehavior1981,schwartzOverviewSchwartzTheory2012}. Furthermore in the reinforcement learning literature there is evidence that given examples of actions a model can learn a opaque value function that can predict what actions will be made next (called inverse reinforcement learning). The methods that are most effective for achieving this is through combining multiple signals (stated values, observed behavior and explicit feedback) \cite{adamowiczCombiningRevealedStated1994}. In machine learning this is done through collaborative learning where different types of value signals at different times are used to train/fine-tune a model \cite{hadfield-menellCooperativeInverseReinforcement2024,ouyangTrainingLanguageModels2022}. Using multiple signal also help mitigate the gap between true values and stated values. Ideally one would just align a model to a persons values then behavior would follow. However due to the gap between values and behavior one needs to provide additional feedback to the model which for LLM is done through preference optimization methods outlined above.

\subsection{Why human values are important for AI alignment}

The is-ought problem is an old philosophical concept that states that one cannot decide what ought to be purely from what currently is \cite{Hume1739Treatise}. This presents a challenge for AI alignment, as we must determine what values we ought to align AI systems to, and all we currently have is what human values currently are. Our models will have some values and some behavior patterns whether we intend them to or not. Whether certain sets of human values are correct or not is a philosophical question that is outside the scope of this work. Yet the distribution of human values is the best target we have to align AI systems to, they might not be the perfect values yet the best target. This is because human values (specifically democratic and representative values) are the closest to a consensus, and therefore under democratic ideals represent the best target for alignment. Alternative methods like inaction letting the model be unaligned and just be what it is or aligning to a specific set of values (e.g.\ expert values) are fraught with issues of bias, lack of representation, and potential for harm. Therefore, aligning AI systems to human values is the most pragmatic and ethical approach to AI alignment.