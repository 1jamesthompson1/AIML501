\section{Large Language Models}

\subsection{Language models}

\textit{Language models are designed to understand and generate text. Generally probabilistic models that learn to predict next tokens. Use a large corpus of text data to learn statistical patterns in language. Started with simple statistical models such as n-grams and evolved to neural network based models such as RNNs, LSTMs and transformers.}

\subsection{Modern Large language models}

\textit{Modern LLMs are transformer based. Just use the decoder. Take the input and generate a sequence of tokens autoregressively. Use self attention mechanism to capture long range dependencies in text. Trained using next token prediction objective on large datasets.
Explain what self attention is (highlevel), how the basic architecture looks (layers of attention, feedfoward, multihead). How generation is done auto regressively. Lastly the training objective of next token prediction and very large corpus of text.}

\subsection{Applications of LLM}

\textit{LLMs are used a lot in modern day. Can achieve a lot. Agentic behavior is common.}

\section{Alignment of Large Language Models}

\subsection{What is alignment?}

As discussed above large language models (LLMs) have shown remarkable capabilities in understanding and generating human-like text. However the pre-training process focuses primarily on predicting the next token in a sequence, this does not necessarily mean that the model will produce useful, safe outputs that align with what users want. Therefore, aligning LLMs is generally \footnote{Other methods are proposed which embed the alignment within the pre-training step itself. Commonly used methods include filtering the training token} the process of taking the pre-trained model and fine-tuning it to better match what the designers and end users expect from the model. The aligning process can work on many dimensions of the models behavior from helpfulness, politeness, honesty, and safety to more complex values such as fairness, inclusively, and ethical considerations. More powerful models have a greater potential to cause harm if not properly aligned, making alignment along the dimensions such as safety and ethical considerations increasingly important. In this section I will outline some of the most common methods for post-training alignment of large language models along with current limitations and other methods.

\subsection{Alignment through preference optimization}
A common element in most post-training alignment methods is the use of preference optimization. The idea is to use a dataset of prompts and potential responses that are ranked either by humans or AI models to indicate which responses are preferred over others. This preference data is then used to fine-tune the pre-trained language model to increase the likelihood of generating preferred responses and decrease the likelihood of generating dis-preferred responses. With the goal that the final model will learn to generalize the preferences to new prompts and situations.

The distribution of prompts in the preference dataset is crucial as it defines the context for which the model is expected to generalize its learned preferences. Therefore it important to ensure that the prompts used to create the preference dataset are sufficiently representative of the contexts and dimensions in which you are expecting the model to align its behavior. Furthermore the rankings of the responses is also important as they define what the model is aligning towards. In many cases these rankings are subjective and provided by a small group of human labelers (n~50).
TODO: Expand this to explain the idea of generalization of preferences more rigorously.

\textbf{Note:}
\textit{Some of these subsection below of specific alignment methods could be compressed into single subsections and only explain the ones that are most relevant to my final proposed method. For now I include the three papers that I have actually read, I expect to compress RLHF and DPO into a "Other alignment methods" subsection. If I use more modern methods such as CAI with KTO or ORPO then I can expand on those more.}

\subsection{Reinforcement Learning from Human Feedback}
Reinforcement Learning from Human Feedback (RLHF) \cite{ouyangTrainingLanguageModels2022} uses human labellers to create the preference dataset and then a reinforcement learning algorithm to optimize the model based on this feedback. This method popularized post-training alignment and was used to train InstructGPT which was smaller and better at following instructions compared to GPT-3 \cite{brownLanguageModelsAre2020, ouyangTrainingLanguageModels2022}.

\subsubsection{How does RLHF work?}

RLHF is separated into three distinct stages. Firstly is supervised fine-tuning (SFT) where a pre-trained language model is fine-tuned on a dataset of prompts and human written 'expected' responses to those prompts. The second step is collecting multiple responses from the SFT model to create a set of prompts and response then having human labelers rank these responses from best to worst. This ranked data is then used to train a preference model (PM) that can predict which of two responses is better aligned to human preferences. Finally the PM is used as a reward signal in the PPO \cite{schulmanProximalPolicyOptimization2017} RL algorithm to further fine-tune the SFT model and create the final RLHF model.

\subsubsection{Relevance}

The idea of using human feedback to help train machine learning models is powerful, although it is not new and was around in theory and concept for years beforehand \cite{christianoDeepReinforcementLearning2017}. The specific method of RLHF was widely adopted \cite{team2023gemini,baiQwenTechnicalReport2023,shaoDeepSeekMathPushingLimits2024} yet is now replaced with more effective methods to utilize human feedback such as Direct Preference Optimization (DPO) \cite{rafailovDirectPreferenceOptimization2024}.

\subsection{Direct Preference Optimization}

The idea presented in \cite{ouyangTrainingLanguageModels2022} introduces the concept of using human feedback to train large language models to be aligned to human preferences. However the process of RLHF is complex and requires training multiple models (SFT, PM, RLHF). Direct preference optimization (DPO) \cite{rafailovDirectPreferenceOptimization2024} is a method that simplifies this process by removing the need for reinforcement learning altogether. Instead DPO reformulates the preference modelling step to instead create an objective function that implicitly creates the reward function from the model and a reference model (usually the SFT model).

\subsubsection{How does DPO work?}

DPO starts with the same preferences data as RLHF; a set of prompts with responses ranked by humans. Like RLHF it starts with a pre-trained language model and is supervised fine-tuned on the prompts and highest ranked responses to create an SFT model. Instead of training a separate preference model, DPO uses the SFT model and the current model to create the DPO update gradient:
\begin{equation*}
\begin{aligned}
\nabla_\theta \mathcal{L}_{\mathrm{DPO}}(\theta)
&= -\mathbb{E}_{(x, y^+, y^-) \sim D} \big[ \\
&\quad
\underbrace{
\sigma \!\left(
\hat{r}_\theta(x,y^-)
-
\hat{r}_\theta(x,y^+)
\right)
}_{\text{Reward model correctness}}
\left(
\underbrace{
\nabla_\theta \log \pi_\theta(y^+|x)
}_{\text{Increase preferred likelihood}}
-
\underbrace{
\nabla_\theta \log \pi_\theta(y^-|x)
}_{\text{Decrease dispreferred likelihood}}
\right)\\
&\big]
\end{aligned}
\end{equation*}

Where $\hat{r}_\theta(x,y) = \log \pi_\theta(y|x) - \log \pi_{SFT}(y|x)$ is the reward function defined as the log difference between the current model and the SFT model, $\sigma$ is the sigmoid function, and $(x, y^+, y^-)$ are the prompt, preferred response, and dis-preferred response respectively. \cite{rafailovDirectPreferenceOptimization2024} found that the reward model correctness weighting term for the update is very important. This update can be calculated with only 4 forward passes through the network per preference pair (2 for reference model and two for current model), making it much more efficient than RLHF \footnote{RLHF will need to do many forward passes of the network to train the preference model, then it will still have to do 3 forward passes for each datapoint when it comes to the RL step.}.

\subsubsection{Relevance}

DPO provides a more effective way to utilize the same underlying feedback data that is used in RLHF. It replaced RLHF as the baseline method for post-training alignment. Due to its widespread adoption DPO represents a strong baseline for alignment.

\subsection{Kahneman-Tversky Optimization}

\cite{ethayarajhKTOModelAlignment2024} sets out to change the objective of a language model to maximize the utility of generations as opposed to maximizing the log-likelihood of preferences. This introduces the concept of Human-aware losses (HALOs) which previous methods like DPO are. \cite{ethayarajhKTOModelAlignment2024} propose a new objective function from the HALO family that works on only binary preference data. This means rather than needing a prompt and at-least two responses (preferred and disprefered), you only need a single response and it will either be desirable response or undesirable response.

\subsubsection{How KTO works}

KTO is built off the foundation of prospect theory \cite{tverskyAdvancesProspectTheory1992} where the human utility of an random situation is determined by a value function and a weighting function. The value function is what determines how good/bad an outcome is compared to a reference point, and the weighting function is used to weight particular outcomes in a biased way in line with how humans perceive probability. KTO translates this theory into an objective function that uses binary preference data and a modified value function. It only needs a database of prompts and responses labelled as either desirable or undesirable ($d=1$ for desirable, $d=-1$ for undesirable). The KTO loss function is defined as:

\begin{equation*}
\mathcal{L}_{\mathrm{KTO}}(\pi_\theta, \pi_{\text{ref}}) = \mathbb{E}_{(x, y, d) \sim D} \left[ \lambda(d) - v(x,y)\right]
\end{equation*}
\noindent where
\begin{equation*}
\begin{aligned}
v(x,y) &=\begin{cases}
\lambda_D \sigma(\beta(r_\theta(x,y)-z_0)), & \text{if } d = 1 \\
\lambda_U \sigma(\beta(z_0 - r_\theta(x,y))), & \text{if } d = -1 \\
\end{cases}
\\
\lambda(d) &=
\begin{cases}
\lambda_D, & \text{if } d = 1 \\
\lambda_U, & \text{if } d = -1 \\
\end{cases}
\\
r_\theta(x,y) &= \log \pi_\theta(y|x) - \log \pi_{\text{ref}}(y|x)\\
z_0 &= D_\text{KL} \left( \pi_\theta(y|x) \| \pi_{\text{ref}}(y|x)\right)
\end{aligned}
\end{equation*}
The three  hyperparameters $\beta$ is a temperature parameter, and $\lambda_D$, $\lambda_U$ are scaling parameters for desirable and undesirable responses respectively. In the references implementation and experiments $\lambda_D = \lambda_U = 1$ and the reference model is a SFT model.

\subsubsection{Relevance}

Collecting the ranked preference data which is needed by normal alignment methods is normally quite expensive \cite{casperOpenProblemsFundamental2023} where binary like dislike information is much more natural and less invasive to collect. KTO is shown to be more efficient and more robust to data imbalances than DPO. Furthermore a model of sufficient size (~13B) does not need a SFT step prior to KTO which greatly reduces computational requirements. Furthermore KTO shows similar performance without using a reference model \footnote{This is done by assuming that the reference model returns a uniform distribution across all outputs given $x$} which also halves the memory requirements when training. 

\subsection{Constitutional AI}

\subsubsection{What is Constitutional AI?}

Constitutional AI (CAI) is a method of post-training a large language model to be aligned to set of principles that are outlined in simple natural language document called a constitution \cite{baiConstitutionalAIHarmlessness2022}. Importantly it works without the need for human labelling of data common in other alignment methods such as RLHF and DPO \cite{ouyangTrainingLanguageModels2022, rafailovDirectPreferenceOptimization2024}. Instead of human labelling CAI uses the model itself to generate feedback on its outputs based on the principles outlined in the constitution, this feedback is then used to further train the model to align it to the constitution.

\subsubsection{How does Constitutional AI work?}

CAI is a process that takes in a constitution, a pre-trained language model, and a dataset of prompts and outputs a model that is aligned to the constitution. The process consists of three main steps: self generative and supervised fine-tuning (SFT), preference modelling (PM), and reinforcement learning with human feedback (RLHF) \cite{baiConstitutionalAIHarmlessness2022}. The constitution itself was generated in an adhoc manner by the authors of \cite{baiConstitutionalAIHarmlessness2022} and consists of a set of principles that aim to make the model's outputs more helpful, honest, and harmless.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
    scale=0.7, transform shape,
    node distance = 2cm and 2.5cm,
    every node/.style={font=\small},
    diamond/.style={draw, shape=diamond, aspect=1.5, align=center, fill=blue!15, blur shadow},
    rect/.style={draw, rectangle, rounded corners=6pt, align=center, fill=red!15, blur shadow},
    smallrect/.style={draw, rectangle, rounded corners=6pt, align=center, fill=yellow!20, minimum width=2.2cm},
    process/.style={draw, rectangle, rounded corners=6pt, align=center, fill=green!20, blur shadow},
    stage/.style={draw, rectangle, rounded corners=6pt, align=center, fill=purple!20, blur shadow},
    arrow/.style={-Latex, thick}
]

% Top left diamond
\node[diamond] (input) {Input pretrained\\Model};

% Harmful sample generation
\node[rect, right=of input] (redteam1) {Generate Responses\\to ``Red Teaming''\\Prompts Eliciting\\Harmful Samples};

% response critique revision boxes
\node[smallrect, above right=0cm and 1.5cm of redteam1] (resp) {Response};
\node[smallrect, below=0.3cm of resp] (crit) {Critique};
\node[smallrect, below=0.3cm of crit] (rev) {Revision};

% SL-CAI diamond
\node[diamond, right=4.8cm of redteam1] (slcai) {Finetuned\\SL-CAI\\Model};

% Lower left harmful pairs
\node[rect, below=2cm of input] (redteam2) {Generate Responses\\to ``Red Teaming''\\Prompts Eliciting\\Pairs of Samples};

% constitutional AI feedback
\node[stage, right=1cm of redteam2] (cai) {Constitutional AI Feedback\\for Self-Improvement};

% preference model
\node[process, right=1cm of cai] (pm) {Finetuned\\Preference Model\\(PM)};

% RL-AIF model block
\node[process, right=1cm of pm] (rlaif) {RLAIF Training with\\PM + SL-CAI\\Models};

% final RL-CAI
\node[diamond, right=1cm of rlaif] (final) {Final\\RL-CAI\\Model};

% arrows
\draw[arrow] (input) -- (redteam1);

\draw[arrow] (redteam2) -- (cai);
\draw[arrow] (cai) -- (pm);
\draw[arrow] (pm) -- (rlaif);

\draw[arrow] (slcai) -- (rlaif);
\draw[arrow] (slcai) -- (redteam2);
\draw[arrow] (rlaif) -- (final);

% arrows from critique-response-revision group
\draw[arrow] (redteam1.east) -- ++(0.8,0) |- (resp.west);
\draw[arrow] (resp) -- (crit);
\draw[arrow] (crit) -- (rev);
\draw[arrow] (rev.east) -- ++(0.7,0) |- (slcai.north);

\end{tikzpicture}

\label{fig:cai_process}
\caption{Overview of Constitutional AI process, adapted from \cite{baiConstitutionalAIHarmlessness2022}}
\end{figure}

\subsubsection{Supervised learning on revised responses}

The first stage of the process involves taking the pre-trained model and using it to generate responses to a set of prompts. These prompts are designed to elicit harmful behavior. As the pre-trained model is designed to be helpful it will likely generate harmful responses to these prompts. These responses are then critiqued by the model itself using principles outlined in the constitution, only a subset of which are used for each response critique. The critique is then used to revise the original response to make it more aligned to the constitution. This process of generating responses, critiquing them, and revising them is done iteratively to create a dataset of revised responses. This dataset is then used to fine-tune the pre-trained model using self-supervised learning to create a model called the SL-CAI model.

\subsubsection{Reinforcement learning with AI feedback}
The next step involves using the SL-CAI model to generate pairs of responses to a set of prompts (overlapping with the set of prompts from before or not). Then using the constitution the SL-CAI model is used to provide feedback on which of the two responses is better aligned to the constitution. This feedback is then used to fine-tune a preference model (PM) that can predict which of two responses is better aligned to the constitution. Finally the PM and SL-CAI models are used together to do reinforcement learning in style of RFHL \cite{ouyangTrainingLanguageModels2022} to generate the final model RL-CAI. This final model is now aligned to the constitution without any human labelling of data. In the setup used in \cite{baiConstitutionalAIHarmlessness2022} the authors did in fact use some labels from human labelers which were used to provide the helpfulness signal in the preference model training, however in principle the preference signal could have been entirely generated by an AI model.

\subsubsection{Relevance}

In the context of when this paper was released there was a key dilemma of helpfulness vs harmlessness in large language models. Therefore, this paper set out to make a model that is both helpful and harmless, without sacrificing one for the other. Further research has expanded the horizons to be concerned with more than just harmfulness vs helpfulness trade off. As hinted at by the authors in \cite{baiConstitutionalAIHarmlessness2022}, there is room for improvement in the CAI method to make it focus on more than just harmlessness vs helpfulness. This can be done by modifying the constitutions to include more diverse principles as well as modifying the red teaming prompts to target more diverse scenarios (i.e controversial topics, political leanings, etc). In this way we can see how elements of CAI can be built upon by generating these diverse constitutions and red teaming prompts in a representative manner to create models aligned to a representative set of values.

\subsection{Current technical alignment limitations}

\textit{Introduce some of the ongoing concerns around current alignment methods. Such as shallow alignment, inner misalignment, scalable oversight etc.}


\section{Broader AI safety concerns}

\textit{
    Take the problem of alignment and expand it broader AI alignment concerns.
    Talk about mis generalization, and specification gaming.
    Introduce the concepts from PublicAI, incentives and collective intelligence.
    ...
}

\section{Representing Human Values}

\textit{
    Discuss concepts such as how does one understand the values of a group. How does it relate to building AI systems. I need to significant reading on this as I am most outside my comfort zone for this.
    Look into some philosophical concepts around how the collective the best baseline if we have no other gold standard. Provide some rationale as to why using the public as the source of values is a good idea, as opposed to some "gold standard" values provided by experts.
}
